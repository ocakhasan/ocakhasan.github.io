[{"categories":null,"content":"Özet Bu yazıda basit bir kod parçasındaki bütün hataları bulup refactor edeceğim. Bunu yaparken de Go dilindeki temel unsurları açıklayarak yapacağım. Bu yazı Concurrency Made Easy videosundan ağır şekilde esinlenmiştir. Go dilinde concurreny baya öne çıkan bir unsur ancak doğru kullanmayı bilmek daha da önemli. Kendim de bu konuda mükemmel sayılmam ancak hala öğreniyorum. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:0:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Elimizdeki Fonksiyon Elimizdeki fonksiyon sadece bir parametre websites alıyor. Bu websiteler üzerinde gezinirken handle diye error döndüren bir fonksiyon alıyor ve handle fonksiyonu herhangi bir error döndürdüğü anda ise bu erroru döndürmek istiyor. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { wg.Done() \u003c-semaphores }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:1:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Sorunlar ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Semaphore ve WaitGroup Kısmı func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { wg.Done() \u003c-semaphores }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } Bu kısımlar kodumuzda bir panic oluşturmuyor, ancak aşağıdaki 2 durumdan birisi oluşuyor. \u003c-semaphores işlemi close(semaphores) işleminden önce oluşabilir ve bu durumda zaten kanaldan bir değer okur. close(semaphores) işlemi daha önce gerçekleşir ve \u003c-semaphores ise zero value alır. Önce wg.Done() operasyonu wg.Wait() fonksiyonun bitmesine ve close(semaphores) satırının çalışmasına yol açabilir. Her iki durumda da bir sıkıntı yok ancak bu kod fonksiyonun takibini daha zor yapıyor. Bunu go dilindeki şu tavsiyeyle çözebiliriz. Release locks and semaphores in the reverse order you acquired them. Anlamı ise locklar ve semaphoreları onları aldığınız sıranın tersinde bırakın. Bu durumda kodumuz şu hale geliyor ve daha basit bir duruma dönüşüyor. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } Şimdi ise sadece tek bir durum gerçekleşebilir o da \u003c-semaphores işlemi channel kapanmadan okuma işlemlerini yapabilir çünkü wg.Wait() işlemi ancak ve ancak bütün semaphores kanalından okuma işlemleri gerçekleştikten sonra gerçekleşebilir. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:1","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Semaphoreların Kullanımı Semaphoreların kullanıldığı kısıma biraz daha yakından bakalım. for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } semaphores channelı 5 uzunluklu bir channel olduğundan dolayı 5 goroutine çalıştıktan sonra 6. taska geldiğinde fonksiyon 2. satırda duracak ve bu handle(website) fonksiyonu bitene kadar durmayacak. Halbuki şöyle bir durum daha mantıklı olabilir. Aynı anda 5 kez handle(website) fonksiyonu çalışsın, bir diğer deyimle goroutineler yaratılsın ve hazırda beklesin. Bunun için şu motto ile hareket edebiliriz. Acquire semaphores when you’re ready to use them. Anlamı ise semaphoreları ne zaman kullanmaya hazırsan o durumda acquire et. for _, website := range websites { go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } Bu değişiklikten sonra artık bütün goroutineler yaratılır ve aynı anda ancak 5 tanesi sadece handle(website) fonksiyonunu çalıştırabilir. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:2","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"For Loop For-range loop da yeni bir değişken website yaratıyoruz. Bir goroutine bu değişkeni updatelerken diğer goroutineler bu değişken üzerinden işlem yapıyor. Bundan dolayı burada bir data race var. Onun yerine 2 şekilde halledebiliriz. Functiona parametre olarak verme for _, website := range websites { go func(website string) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }(website) } Yeni Değişken Olarak Tanımlama for _, website := range websites { website := website go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } Bundan ayrı olarak da genelde goroutineleri ayrı fonksiyonlara almak önerilir. Bu kod parçasını go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() şu şekilde refactor edebiliriz. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { go worker(website, semaphores, \u0026wg, errChan) } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } func worker(website string, sem chan struct{}, wg *sync.WaitGroup, errChan chan err) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } } ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:3","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Error Channele Yazma Bütün bu işlemleri yaptık ancak hala kodumuzda bir sorun var. Herhangi bir goroutine errChan \u003c- err işlemini yaptığında diğer bütün error goroutineler bu kanala yazarken sonsuza kadar bekleyecekler ve bu da deadlock yaratacak. Bekleme sebebi errChan kanalının 1 uzunlukta bir channel olmasından dolayıdır. Bir goroutine başlatmadan önce ne zaman ve nasıl duracağını bilmek gerekir. Bunun yerine select ve case kullanarak sorunu halletmiş oluruz. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { go worker(website, semaphores, \u0026wg, errChan) } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } func worker(website string, sem chan struct{}, wg *sync.WaitGroup, errChan chan err) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { select { case errChan \u003c- err: default: } } } Bu durumda eğer herhangi bir goroutine errChane yazabilirse yazacak ve yazamazsa default case çalışacak. Hiçbir goroutine bloklanmayacak. Select Case ile blocking çağrıları non-blocking olarak değiştirebiliriz. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:4","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"REFERENCES Concurrency Made Easy From Dave Chevey ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:5","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Bu yazıdaki bütün kodlar Bu repodan bulunmaktadır. Eğer demo versiyonunu görmek isterseniz http://banafilmoner.herokuapp.com/ sitesinden görebilirsiniz. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:0:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Gereksinimler Bu yazımızda yapacağımız siteyi eğer kendiniz de yapmak istiyorsanız Flask ve Scikit-learn kütüphanelerini yüklemeniz gerekmektedir. Bunları yüklemek için terminalden şu komutları yazabilirsiniz ya da her bir paketin dökümentasyonundan bakabilirsiniz. pip install Flask pip install scikit-learn ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:1:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Sitenin Yapısı Yapacağımız sitede film önerileri metin benzerliği ile olacak. Bu filmlerin açıklama metinlerini ise bir veri kümesinden alacağız. Bu veri kümesine TMDB 5000 Movies sayfasından ulaşabilirsiniz. Bundan dolayı önerebileceğimiz metinler sadece bu veri kümesindekiler olacaktır. Metin benzerliğini ise kosinüs benzerliği ile yapacağız. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:2:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Veri Seti ve Metin Benzerliği Veri setindeki title sütunu filmin başlığını ve overview sütunu ise filmi basitçe açıklar.Bu yazıda overview sütununu kullanarak metin benzerliğini kuracağız. Bunun için önce utils.py diye bir dosya oluşturalım ve indirdiğimiz veri setini de projedeki dosyaya koyalım. Öncelikle filmlerin açıklamalarını kullanarak kosinüs benzerliğini verecek olan bir fonksiyon yazalım. from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import linear_kernel def get_cosine_similarities(df): vectorizer = TfidfVectorizer(stop_words=\"english\") tf_idf_mat = vectorizer.fit_transform(df['overview']) cosine_sim = linear_kernel(tf_idf_mat, tf_idf_mat) return cosine_sim get_cosine_similarities(df) fonksiyonu parametere olarak DataFrame alır, DataFramei ise veri setini okuduktan sonra bu fonksiyona parametre olarak vereceğiz. Fonksiyonda kullanılan TfidfVectorizer metinlerden bilgi çıkarmamıza yarayan bir algoritmadır. Açılımı Term frequency (tf) -\u003e (terim sıklığı) ve inverse document frequency (ters döküman sıklığı)dır. Yani terimlerin her bir metinde ne kadar sıklıkla geçtiğine ve bu terimlerin bütün dökümanda ne kadar sıklıkla geçtiğine bakıp, hangi terimlerin cümleleri ayırmada önemli olduğuna karar verir. Bu bize (4803, n) boyutunda bir matrix dönderecektir. n ise bu algoritmanın bulduğu belirleyici kelimelerin sayısıdır. Yazdığımız fonksiyonla beraber, her bir cümle için her bir kelimenin ne kadar önemi olduğunu gösteren bir matrix elde edilecek. Daha sonra bu matrixi kullanarak her bir metin arasındaki benzerliği bulmak için linear_kernel kullanıyoruz. Bu algoritma ise bize (4803, 4803) boyutunda her bir metnin diğer 4038 filmin metni ile benzerliğini gösteren bir matrix döndürecek. Bu fonksiyondan çıkan sonuç ise şu şekildedir [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0.02160533 0. 0. ] [0. 0. 1. ... 0.01488159 0. 0. ] ... [0. 0.02160533 0.01488159 ... 1. 0.01609091 0.00701914] [0. 0. 0. ... 0.01609091 1. 0.01171696] [0. 0. 0. ... 0.00701914 0.01171696 1. ]] Görüldüğü gibi bazı değerler 0 bazıları 1 (köşegendekiler), bazıları da 0 ile 1 arasında. Kısaca Sonucu 0 olanlar arasında hiçbir benzerlik yok, 1 olanlar zaten kendileri ile ölçüldüğü için aynı olarak çıkıyor, örnek olarak 1.film ile 1.film arasındaki benzerlik 1 olacak doğal olarak 0-1 arasındakiler ise iki film arasındaki benzerliği gösteriyor. Ne yaptığımızı kısaca yazalım. Veri setini okuduk Kosinüs benzerlik matriksini oluşturduk. Şimdi yapılması gerekenler ise bu matrixi kullanıp film önerileri alabilmek. Bunun için yapılması gerekenler Kosinüs matrixini kullanıp bize verilen film için önerileri döndüren bir fonksiyon yazmak Flask ile web arayüzü oluşturup, kullanıcın girdiği filme öneriler vermek Bu fonksiyonu flask ile kullanabilmek. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Film Önerme Fonksiyonu Bu fonksiyona geçmeden önce veriyi okuyalım, ve kosinüs matriximizi alalım. Şunu belirtmem gerekir ki, kullanıcının attığı her requestte veri setini baştan okuyup kosinüs matrixini okumak yük olur. Bundan dolayı, bunu bir kez yapmak adına, bu işlemleri if __name__ == \"__main__\" altında yapacağız. Öncelikle bir app.py adında bir dosya açalım. Bu dosyada Flask applikasyonumuzun kodları olacak. Diğer utils.py dan fonksiyonları çağıracağız. app.py dosyasına şu kodları girelim. from flask import Flask, render_template, request, redirect import pandas as pd import utils app = Flask(__name__) if __name__== \"__main__\": df = pd.read_csv(\"data.csv\") df['overview'] = df['overview'].fillna('') df['lower_name'] = df['title'].str.lower() titles = pd.Series(df.index, index=df['lower_name']).drop_duplicates() cosine_sim = utils.get_cosine_similarities(df) app.run() Şuan app.py dosyasında yapılan işlemler. Flask uygulaması oluşturuldu. Veri okundu. Kosinüs benzerlik matriksi oluşturuldu. Main kısmında titles diye bir değişken oluşturulma sebebi bu değişkenin filmleri önerecek olan fonksiyonda kullanacağımızdan dolayıdır. Titles değişkeni tip olarak Seriesdir. Konsola yazdırdığımız zaman şöyle bir sonuç çıkacaktır. lower_name avatar 0 pirates of the caribbean: at world's end 1 spectre 2 the dark knight rises 3 john carter 4 ... el mariachi 4798 newlyweds 4799 signed, sealed, delivered 4800 shanghai calling 4801 my date with drew 4802 Length: 4803, dtype: int64 Şimdi filmleri önerecek fonksiyonu yazmaya başlayabiliriz. Bunu utils.py dosyasında yazalım. \"\"\" movie_title = istenilen filmin ismi cosine_similarity = kosinüs benzerlik matriksi titles= az önce oluşturduğumuz filmin isimlerine sahip olan `Series` df = bütün filmleri barındıran dataframe \"\"\" def get_recommendations(movie_title, cosine_similarity, titles, df): index_movie = titles[movie_title] #istenilen filmin indexini bul name_of_movie = df.iloc[index_movie]['title'] #daha sonra dataframeden filmin adını bul. #istenilen isim küçük harfli olabilir, biz #dataframde nasılsa onu almak için yapıyoruz. similarities = cosine_similarity[index_movie] #daha sonra girilen filmin kosinüs benzerlik #arrayini al, diğer filmlerle benzerlik arrayi similarity_scores = list(enumerate(similarities)) #işlem kolaylığı için her bir benzerliğin indexini #alabilmemiz lazım. yani (0, 0.2), (1, 0.4), (2. 0.7) ... gibi. similarity_scores = sorted(similarity_scores , key=lambda x: x[1], reverse = True) #bütün benzerlik skorlarını sırala similarity_scores = similarity_scores[1:11] #en benzer 10 filmi al similar_indexes = [x[0] for x in similarity_scores] #benzer filmlerin indexlerini al return df.iloc[similar_indexes], name_of_movie #benzer filmlerin bilgilerini almak için indexlerini kullan. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:1","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"HTML Arayüz Bu fonksiyon da yazıldığına göre şimdi Flask ile bağlayabiliriz. Ama öncelikle bir arayüzümüz olması gerekiyor. Bunun için aynı klasörde templates diye bir klasör oluşturun ve içine index.html adında bir dosya oluşturun. Bu dosya bizim kullanıcıdan arayüzü almamızı sağlayacak olan HTML kodunu içerecek. HTML kısmını anlatmayacağım. Basit şekilde Flask bildiğinizi varsayıyorum. index.html dosyasına buradaki arayüz kodunu yapıştırın. HTML kısmı şuan çok ilgi alanımız değil, eğer arayüz nasıl görünüyor diye merak ediyorsanız, buradan bakabilirsiniz. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:2","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Flask Endpointleri halletme Bu kodda dikkatinizi çekmek istediğim bir nokta var. FORM bir ‘/’ yoluna POST request yapıyor. Flask uygulamasında ‘/’ adresine bir POST request yapılacak. Ayrıca websitesinin giriş sayfası da bu adrese GET request yapılarak alınacak. Şimdi app.py dosyasında bu koşulları sağlayan kodumuzu yazalım. from flask import Flask, render_template, request, redirect, flash, url_for import pandas as pd import utils app = Flask(__name__) @app.route('/', methods=['GET', 'POST']) def hello(): length = 0 movie_name = \"\" context = { #Bu dictionary önerilen filmlerin bilgilerini tutuyor. 'movies': [], #isimler 'urls': [], #filmlerin sayfaları 'release_dates': [], #filmlerin yayınlanma tarihleri 'runtimes': [], #filmlerin süreleri 'overviews': [] #filmleri anlatan metinler } if request.method == \"POST\": #Kullanıcı bir input girdiyse text = request.form['fname'].lower() print(\"request text\", text) try: recommended_df, movie_name = utils.get_recommendations( text, cosine_sim, titles, df) #girilen inputtan filmleri al context['movies'] = recommended_df.title.values context['urls'] = recommended_df.homepage.values context['release_dates'] = recommended_df.release_date.values context['runtimes'] = recommended_df.runtime.values context['overviews'] = recommended_df.overview.values length = len(context['movies']) except: return render_template('index.html', error=True) #filmi bulamadıysak error döndür. return render_template('index.html', length=length, context=context, movie_name=movie_name, error=False) if __name__ == '__main__': df = pd.read_csv(\"data.csv\") df['overview'] = df['overview'].fillna('') titles = pd.Series(df.index, index=df['lower_name']).drop_duplicates() cosine_sim = utils.get_cosine_similarities(df) app.run() Render templatede gönderilen context değişkeni HTML dosyasında parse ediliyor ve bilgiler güzel bir şekilde gösteriliyor. Dediğim gibi basit şekilde Flask bildiğiniz düşünüyorum. Beğendiyseniz paylaşırsanız çok sevinirim. İyi öğrenmeler. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:3","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"TANIM PyTorch da bulunan torch.autograd otomatik türev alma motoru şeklinde çalışır ve bu da nöral ağ eğitimini güçlendirir. Bu yazımızda belirli örnekler vererek konunun daha geniş şekilde anlaşılmasını sağlayacağız. Öncelikle çok kısa bir özetleyici metine bakalım. ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:1:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Arka Plan Nöral ağlar (neural networks) kendisine verilen veriyi belirli fonksiyonlarda işleyen bir bütündür. Bu fonksiyonların her biri bazı parametrelerden (ağırlıklar ve önyargı (weights and bias)) oluşur. Bu belirlenen parametrelere Pytorch da tensor adlı veri yapılarında tutulur. Bir Nöral ağın eğitilmesi iki kısımdan oluşur. Birinci kısımda sadece ileriye gidilir (forward propagation) ve ikinci kısımda geriye doğru gidilir (backward propagation). Peki bu ileri ve geri gitme işlemleri ne için yapılır onlara bakalım. İleriye Gitme (Yayılma) Bu kısımda nöral ağ kendisine verilen veriden en iyi tahminini yapmaya çalışır. Bu belirlenen veri, önceden bahsettiğimiz her bir fonksiyondan geçer ve en sonunda bir tahmin ortaya atılmış olur. Daha sonra belirlenen tahmin ve gerçek değer arasından bir kayıp (loss) değeri bulunur ve hatta bu değeri bulan fonksiyona da loss function denilir. Geriye Gitme (Yayılma) Bu kısımda ise nöral ağ, ilk bölümde hesaplanan kayıp veya hata değerini azaltmaya yönelik parametrelerinde iyileşmeye gider. Bunu yaparken de sonuçtan geriye dönük olarak her hata değerinin her bir parametreye bağlı olan türevini (derivative) hesaplar ve bu parametreleri, gradient descent kullanarak optimize eder. Ancak bu her bir fonksiyonun parametrelere göre türevini tek tek elimizle alamayız ve bize otomatik bir süreç lazım. İşte bu kısımda pytorch.autograd devreye giriyor ve bütün yükü alıyor. ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:1:1","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Autograd’da Türev Alma İşlemleri Şimdi autograd‘ın bütün bu değerleri nasıl kayıt ettiğine bakalım. Öncelikle iki tane a ve b tensor oluşturalım. Bu tensorları oluştururken parametre olan requires_grad parametresini True yapmamız gerekiyor aksi halde otomatik türev alma işlemleri gerçekleşemez çünkü bu parametre Tensorun grad adlı attributunda bu değerleri kayıt etmemize yardımcı oluyor. import torch x = torch.tensor([1., 2.], requires_grad=True) y = torch.tensor([2., 4.], requires_grad=True) Şimdi bu iki tensoru kullanarak yeni bir tensor z oluşturalım. Basit şekilde formül $$ z = 6x^2 - 2b^3 $$ z = 6*x**2 - 2*y**3 Şöyle varsayalım, x ve y bizim parametrelerimiz ve z bizim hata fonksiyonumuz olsun. Nöral ağ eğitiminde, hatanın bu parametreleri bağlı olan gradyantlarını (gradient) isteriz. PyTorch’da .backward() fonksiyonunu çağırdığımız zaman, auutograd her bir parametrenin (x, y) gradyantlarını bulur ve bunları her bir tensorun .grad attributunda kayıt eder. Öncelikle şuan x ve y nin grad değerlerine bakalım. print(\"X.grad = \", x.grad) print(\"Y.grad = \", y.grad) #Output X.grad = None Y.grad = None Ancak şimdi z tensorunda .backward() çağırdığımız zaman x ve y nin .grad attributularında z'nin kendilerine göre türevler yer alacak. Ancak z.backward() argümanını çağırabilmek için parametre olarak gradyant (gradient) vermemiz gerekiyor çünkü z bir vektör. Gradyant z ile aynı boyutlara sahip ve z’nin z ye göre türevini temsil eder. Şimdi z.backward() fonksiyonunu çağırabiliriz. gradyant_parametre = torch.tensor([1., 1.]) z.backward(gradient=gradyant_parametre) Şimdi x.grad ve y.grad değerleri oluşacak. Ancak bu değerleri görmeden önce kendimiz basit bir türev alalım. $$ \\frac{\\partial z}{\\partial x} = 12x $$ $$ \\frac{\\partial z}{\\partial y} = -6y^2 $$ Daha sonra bu kısmi türevlere x ve y tensorlarını koyduğumuz zaman ortaya çıkacak sonuçların şu şekilde olması lazım. print(\"x için = \", 12 * x) print(\"y için = \", -6 * y**2) x için = tensor([12., 24.], grad_fn=\u003cMulBackward0\u003e) y için = tensor([-24., -96.], grad_fn=\u003cMulBackward0\u003e)) Şimdi basit bir şekilde kontrol edelim. print(\"x.grad = \", x.grad) print(\"y.grad = \", y.grad) x.grad = tensor([12., 24.]) y.grad = tensor([-24., -96.]) Gördüğümüz üzere sonuçlar doğru çıkıyor. Üstte gözüken ggrad_fn=\u003cMulBackward0\u003e ise bu bu tensorun nasıl bir matematiksel operatör kullanarak oluşturulduğunu söylüyor. Eğer required_grad=False olsaydı bu değer None olurdu. Bütün yazdığımız operasyonlar için required_grad=True idi. Şimdi required_grad=False yapıp bir de öyle deneyelim. x = torch.tensor([1., 2.], requires_grad=False) y = torch.tensor([2., 4.], requires_grad=False) z = 6*x**2 - 2*y**3 gradyant_parametre = torch.tensor([1., 1.]) z.backward(gradient=gradyant_parametre) print(\"X.grad = \", x.grad) print(\"Y.grad = \", y.grad) Bu işlemden şöyle bir sonuç alacaksınız. RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn Bu da demek oluyor ki x ve y nin grad değerleri yok ve bundan dolayı grad_fn fonksiyonları da yok. x ve y nin grad değerleri olmadığı için z‘nin de grad değeri yok ve bu da hataya yol açıyor. Derin öğrenmede genellike önceden belirli datasetler ile eğitilmiş hazır modeller bulunmaktadır ve bunlara pretrained model denir. Bu modelleri kullanırken genellikle son katmana kadar olan bütün katmanların parametrelerini eğitmek istemeyiz çünkü bu işlem hem pahalı hem de çok da gerekli olmayan bir işlem. Bu parametreleri optimize etmeye çalışmadığımızdan dolayı bu parametrelerin required_grad değerleri False olacaktır. Örnek bir kod olarak da from torch import nn, optim model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False Burada Resnet18 modelinin parametrelini dondurma (freeze) işlemi yapılıyor ve böylece modeli kullanırken resnet18 modelini parametrelerinde herhangi bir optimize etme durumu söz konusu olmayacak. Ancak, sonradan eklenilen layerlarda optimize çal","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:2:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"REFERENCES Pytorch Tutorials ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:3:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Yazıya başlamadan önce belirmek isterim ki, bu tarz derin öğrenme terimlerinin İngilizce ile kullanılması taraftarıyım. Teknik terimlerin Türkçe karşılıkları genelde her zaman duymadığımız kelimeler oluyor ve internette Türkçe pek kaynak yok. Ondan dolayı ben bu terimlerin İngilizce öğrenilip, İngilizce kullanılması taraftarıyım. Herkes global olmaya çalışırken, bizim öyle davranmamamız için hiçbir sebep yok. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:0:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"TANIM Convolutional sinir ağları genel olarak sıradan sinir ağlarına çok benzerdir. Bu sinir ağları da öğrenebilir ağırlık (weight) ve önyargısı (bias) olan sinirlerden (neuron) oluşur. Her bir nöron bazı inputlar alır, dot product uygular ve bu işlemi lineer olmayan bir yolla devam ettirir. Bütün network hala tek bir ayırt edilebilir skoru açıklar. Network resim pixellerini alıp, sonda bir tahmin üretir. Networkun sonunda belirli bir kayıp fonksiyonu (loss function) bulunur. Peki bu convolutional sinir ağları normal sinir ağlarına bu kadar benziyorsa ne değişiyor? Bu sorunun cevabı ise şu şekildedir: Convolutional sinir ağları inputun resimlerden oluştuğunu varsayar, bu varsayım bize bazı özellikleri sisteme entegre etmemize yardımcı olur. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:1:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"YAPISAL GÖZLEM Normal Sinir Ağları: Normal sinir ağları tek bir input alır, onu bazı gizli katmanlardan (hidden layer) geçirir. Her bir hidden layer nöron kümelerinden oluşur, her bir nöron, bir önceki katmandaki bütün nöronlarla bağlantılıdır ve diğer nöronlardan bağımsız şekilde çalışır. Son katman ise sonuç katmanı (output layer) olarak adlandırılır ve bu katmanda her bir sınıfın olasılığı belli olur. Bu normal sinir ağları resimler kullanılınca pek iyi ölçeklenemiyor. Örnek olarak $(32, 32, 3)$ lük boyutlarda resimler kullanırsak, ilk katman $32 * 32 * 3 = 3072$ ağırlığa sahip olacaktır. Bu yük halledilebilir şekilde görülüyor ancak, bu fully-connected yapı büyük resimlere ölçeklenmiyor. Örnek olarak eğer biz boyutları $(200, 200, 3)$ olan resimler kullanırsak, bu sefer nilk nöronlar $200 * 200 * 3 = 120, 000$ ağırlığa sahip olacaklar. Ancak bu büyük numaralı ağırlıklar aşırı uyma (overfitting) denilen olaya sebep olacaktır. Convolutional sinir ağları ise inputun resimlerden oluşmasınından faydalanır ve buna göre yapıyı daha mantıklı şekilde kurar. Normal sinir ağlarının aksine, Convolutiona sinir ağlarının nöronları 3 boyuta ayarlanmış şekildedir. genişlik, yükseklik, derinlik. Örnek olarak $(32, 32, 3)$ boyutlu resimlerde Genişlik = 32 Yükseklik = 32 Derinlik = 3 olacaktır. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:2:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"PEKI BU CONVOLUTIONAL SINIR AĞLARI NASIL OLUŞTURULUYOR? Bu sinir ağları katman dizilerinden oluşur ve bu katmanlar ise şu şekildedir. Convolutional Katman Pooling Katmanı Fully-Connected Katmanı Bu 3 katmandan oluşan katmanları birleştirip bir sinir ağı oluşturacağız. CONVOLUTIONAL KATMAN Convolutional katman Convolutional sinir ağlarının büyük ağır işini yapan katmanlardır. Conv katmanlar parametreleri öğrenilebilir filtrelerden oluşur. Her bir filtre boyut olarak küçüktür, ancak input derinliği boyunca uzanırlar. Örnek olarak, tipik bir filtre $5 * 5 * 3$ boyutlarında olabilir. İlk 5 genişlik, ikinci 5 yükseklik ve üçüncü 3 ise resimin 3 derinlikli olmasından kaynaklanır. Doğrudan iletme kısmında, her bir filtreyi input resmi üzerinde kaydırıyoruz, bu kaydırma sırasında resimlerde pixeller ile filtredeki sayılar ile dot product alıyoruz. Filtreyi kaydırma işlemi sırasında 2 boyutlu bir aktivite haritası oluşturuyoruz. Bu harita ise bize her bir pozisyondaki cevabı veriyor. Sinir ağı, bu filtreler ne zaman belirli bir görsel özellik, örnek olarak kenar, gördüğü zaman öğrenecek. Her bir filtrenin oluşturduğu haritaları üst üste sıkıştırıp bunu bir sonraki katmana iletiyoruz. BOYUTSAL AYARLAMA Her bir nöronun nasıl bağlı olduğunu anlattık ancak output hacminde kaç tane nöron olduğundan bahsetmedik. Output hacmini belirleyen 3 ayrı parametre vardır. DERİNLİK: Bu parametre kaç tane nöron kullandığınıza işaret eder. Örnek olarak ilk convolutional katman input olarak resmi alırken, farklı nöronlar bu resimde farklı detayları fark edebilir. STRIDE (KAYDIRMA ADIMI): Bu parametre ise filtreyi kaç pixel kaydıracağımıza işaret eder. Eğer stride bir ise, filtreleri bir pixel kaydıracağımız anlamına gelir. ZERO-PADDING: (SIFIRLARLA DOLDURMA) Bazı durumlarda inputun etrafını sıfırlarla doldurmak uygun olmaktadır. Bu işlemin güzel bir tarafı ise, bize output boyutunu kontrol altında tutma olanağı vermesidir. Örnek olarak daha yüksek boyutlu outputlar istersek, inputu filtre boyutu kadar sıfırlarla doldurup, bir sonraki katmana aktarılacak outputun boyutunu, şuanki katmandaki input boyutuna eşit tutabiliriz. Output hacminin boyutunu şu şekilde hesaplayabiliriz. Input Boyutu = $W$ Convolutional katman nöronları filtre boyutu = $F$ Stride = $S$ Zero-Padding = $P$ Output hacmi boyutu formülü = $(W - F + 2P) / S + 1$. Örnek olarak eğer elimizde $10 * 10$ boyutlu bir resim varsa ve bizim filtre boyutumuz $3 * 3$, stride = $1$ ve padding = $0$ ise $$ Output Boyutu = (10 - 3 + 2*0) / 1 + 1 = 8 * 8 $$ Şimdi bu boyut tek bir nörondan çıkan sonuç. Eğer elimizde $n$ tane nöron varsa, bu katmandan çıkan sonucun boyutu $8 * 8 * n$ olacaktı. Yukarıdaki örnekten de görüleceği üzere filtre boyutumuz $3 3$, bundan dolayı resimde de (33) lük alanlar alıp, bu aldığımız alanla filtre arasında bir dot product işlemi uyguluyoruz. Peki resimdeki $31$ sayısına nasıl ulaştık onu inceleyelim. $$ (1 * 1) + (0 * 2) + (1 * 3) + (0 * 4) + (1 * 5) + (1 * 6) + (1 * 7) + (0 * 8) + (1 * 9) $$ $$ 1 + 3 + 5 + 6 + 7 + 9 = 31 $$ Özetlemek gerekirse Conv layer $W_1 * H_1 * D_1$ boyutlarında input alır 4 parametreye ihtiyaç duyar Filtre sayısı $K$ Filtrenin boyutları $F$ Stride $S$ Zero padding sayısı $P$ $W_2 * H_2 * D_2$ boyutlarında output üretir. $W_2 = (W_1 - F + 2P)/S + 1)$ $H_2 = (H_1 - F + 2P)/S + 1$ $D_2 = K$ ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:3:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"PYTHON İLE UFAK BİR GÖSTERİM Şimdi tensorflow ile basit bir gösterim yapıp bu boyutları daha iyi anlayalım. import tensorflow as tf # The inputs are 28x28 RGB images with `channels_last` and the batch # size is 4. input_shape = (4, 28, 28, 3) x = tf.random.normal(input_shape) y = tf.keras.layers.Conv2D( 2, 3, activation='relu', input_shape=input_shape[1:])(x) print(y.shape) (4, 26 , 26, 2) Burada olan işlemler şu şekildedir input_shape Conv layer’a verilecek olan inputun boyutlarıdır. (4, 28, 28, 3) şu anlama gelmektedir. Bizim elimizde 4 adet resim var, ve bu resimlerin boyutları (28, 28, 3)tür. Conv2D ’ e verilen parametreler ise şu şekildedir. İlk verilen parametre 2 kaç adet filtre kullanacağımızı gösterir. İkinci parametre 3 ise filtre boyutunu vermektedir. Yani filtre boyutumuz $(3, 3)$ olacaktır. Şimdi burada oluşan outputun nasıl oluştuğuna bakalım. Yukarıda özetlediğimiz gibi her şeyi tek tek yazalım Input boyutları $W_1 * H_1 * D_1$ şeklindeydi. Bundan dolayı $W_1 = 28$ $H_1 = 28$ $D_1 = 3$ Daha sonra filtre sayımız $K = 2$, filtre boyutumuz ise $F = 3$, stride ise default olarak $S = 1$dir. Padding ise default olarak $P = 0$dır. O zaman şimdi output boyutlarımızı $(W_2 * H_2 * D_2)$ hesaplayabiliriz. $W_2 = (28 - 3 + 2 * 0) / 1 + 1 = 25 + 1 = 26$ $H_2 = (28 - 3 + 2 * 0) / 1 + 1 = 25 + 1 = 26$ $D_2 = K = 2$ Her bir resim için oluşturulan output boyutları $(26, 26, 2)$. Elimizde 4 adet resim var ve bundan dolayı çıkan output boyutu $(4, 26, 26, 2)$ ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:3:1","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"POOLING LAYER Convolutional sinir ağlarında convolutional katmanlar arasına Pooling katmanları eklemek çok yaygındır. Pooling katmanının görevi verilen inputun boyutlarını kademeleri olarak azaltarak parametrelerin ve ağın işlem yükünün azaltımasını sağlamak. Bu şekilde aşırı uyma (overfitting) kontol altına alınmış olur. Pooling katmanı, bağımsız olarak çalışır ve her bir inputu Max operasyonu kullanarak boyutlarını azaltır. En yaygın Pooling katmanı, filtreleri $(2 * 2)$ boyutlarında olan ve inputu hem boydan ve hem enden ikiye bölenlerdir. Her bir Max operasyonu input olarak $(2 * 2)$ lik bir bölüm alacak ve bu 4 sayıdan en büyüğünü gönderecektir. Özetlemek gerekirse, Pooling katmanı input olarak $W_1 * H_1 * D_1$ boyutlarını kabul eder. İki parametreye ihtiyaç duyar Boyut $F$ Stride $S$ Boyutları $W_2 * H_2 * D_2$ olan output çıkarır. $W_2 = (W_1 - F)/S + 1$ $H_2 = (H_! - F)/S + 1$ $D_2 = D1$ Resimde de görüleceği üzere her bir $(2 * 2)$ lik bölümden en büyük sayılar alınıp yeni bir örnek elde ediliyor. Şimdi bunu Python ile kodlamaya çalışalım. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:4:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"Pooling Layer Python İle İmplementasyonu Bu layerı hem sıfırdan hem de kütüphane kullanarak kodlayabiliriz. Önce kütüphane kullanarak gösterelim. x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) x = tf.reshape(x, [1, 3, 3, 1]) max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid') max_pool_2d(x) Bu koddan çıkacak output ise \u003ctf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy= array([[[[5.], [6.]], [[8.], [9.]]]], dtype=float32)\u003e Çıkan sonucun nasıl çıktığını bence rahatlıkla yapabilirsiniz. Şimdi kendimiz sıfırdan bu layerı basit bir şekilde implement edelim. import numpy as np def pool2d(X, pool_size, mode='max'): p_h, p_w = pool_size #pool size ı al Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) #Outputu oluştur for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = X[i: i + p_h, j: j + p_w].max() #Her bir pool size kadar pixelin max'ını al return Y Şimdi kodumuzu yukarıda yazdığımız x arrayi ile test edersek, yine aynı sonucun çıkacağını göreceğiz. Bu yazımızda konuşulacaklar bu kadar. Beğendiyseniz paylaşmayı unutmayın. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:4:1","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"REFERENCES https://anhreynolds.com/blogs/cnn.html https://cs231n.github.io/convolutional-networks/ https://cezannec.github.io/Convolutional_Neural_Networks/ https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D https://medium.com/ai-in-plain-english/pooling-layer-beginner-to-intermediate-fa0dbdce80eb ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:5:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"Makine öğrenmesinde modellerin veriyi görme şekli biz insanlardan farklıdır. Biz kolayca Kırmızı arabayı görüyorum. cümlesini anlayabilirken, model bu kelimeleri anlayacak vektörlere ihtiyaç duyar. Bu vektörlere word embeddings denir. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:0:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"WORD VECTORLERİ NASIL ÇALIŞIR - Tablodan Bak Her kelimemiz için belirli bir boyutta vektörümüz olacak ve bu vektörleri kelimeyi isteyerek alabiliriz. Buna key-value pair örneği verilebilir. key: kelime value: vektör Bundan dolayı herhangi bir kelimenin vektörüne bakmak için dictionaryden kelimeyi istediğimiz zaman vektöre ulaşmış olacağız. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:1:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Word2Vec: Tahmin Bazlı bir Metod Ana amacımız kelimelerden, kelime vektörleri oluşturmak. Word2Vec parametreli word vektörleri olan bir modeldir. Bu parametreler itaretive yöntemle, objective function(küçültmeye çalıştığımız fonksiyon) kullanarak optimize edilir. Peki bunu nasıl yapacağız. Unutmadan: amaç : her bir vektörü kelimenin içeriğini bilecek şekilde kodlamak nasıl yapılacak: vektörleri kelimelerden olası içerik tahmin edecek şekilde eğitmek. Word2Vec iterative bir metottur. Ana fikirleri kısaca şöyledir. büyük bir text corpusu alır texti, belirli bir sliding window(kayan pencere) kullanarak, her seferinde bir kelime ilerleyecek şekilde ilerlemek. Her bir adımda, bir tane central word (merkezi kelime) ve context words(içerik kelimeleri) -\u003e penceredeki diğer kelimeler. merkezi kelime için, içerik kelimelerinin olasılıklarını hesapla. vektörleri olasılıkları artıracak şekilde ayarla Resimde de görüleceği üzere her seferinde arkası mavi olan merkezi kelime ve diğerleri de içerik kelimeleri. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Objective Function (Amaç Fonksiyonu) Text corpusundaki her bir $ t = 1, … , T$ pozisyon için, Word2Vec merkezi kelimesi $w_{t}$ verilmiş m-boyutlu penceredeki içerik kelimelerini tahmin eder. $$ Likelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{t + j} \\mid w_t, \\theta) $$ Bu fonksiyonda $\\theta$ optimize edilecek bütün parametrelerdir. Amaç ve Kayıp Fonksiyonu $J(\\theta) ise ortalama negatif log olabilirlik fonksiyonudur. (Negative log-likelihood) $$ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0 } \\log P(w_{t + j} \\mid w_t, \\theta) $$ Bu formüldeki parçalara ayıralım. $\\sum_{t=1}^{T}$ Bu kısım bütün text üzerinde gezinir. $\\prod_{-m \\leq j \\leq m, y \\neq 0}$ bu ise kayma penceresini(sliding window) temsil eder. $\\log P(w_{t + j} \\mid w_t, \\theta)$ : bu ise merkezi kelimesi verilen içeriğin olasılığını hesaplar. Peki asıl sorulması gereken soru bu olasılıklar nasıl hesaplanacak? ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:1","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Olasılıkları Nasıl Hesaplayacağız? Hesaplamak istediğimiz olasılık $$ P(w_{t + j} \\mid w_t, \\theta) $$ Verilen her kelime $w$ için, iki adet vektörümüz var. $v_w$ -\u003e kelimenin merkezi kelime (central word) olduğu zaman $u_w$ -\u003e kelimenin içerik kelime (context word) olduğu zaman Vektörler train edildikten sonra, genel olarak içerik vektörlerini $u_w$ atar ve sadece merkezi kelime vektörlerini $v_w$ kullanılır. Bundan sonra verilen merkezi kelime $c$ ve içerik kelimesi $o$ kelimeleri için olasılık: $$ P(o \\mid c) = \\frac{exp(u_{o}^{T})}{\\sum_{v \\in V} exp(u_{w}^{T} v_c)} $$ NOT: Bu bir softmax fonksiyonudur. Softmax ile alakalı yazıma bu yazımdan ulaşabilirsiniz. Şimdi bu olasılıkları nasıl hesaplayacağımız gördüğümüze göre, vektörleri nasıl eğiteceğimizi görelim. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:2","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"NASIL EĞİTİLİR Kısaca bu sorunun cevabı Gradient Descent ile her seferinde bir kelime alarak gerçekleşir. Parametrelerimiz $\\theta$ bütün kelimelerin $v_w$ ve $u_w$ vektörleri olduğunu hatırlayalım. Bu vektörleri gradient descent kullanarak optimize edeceğiz. $$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta} J(\\theta) $$ Bu parametre güncellemerini her seferinde bir kelime kullanarak yapıyoruz. Her bir güncelleme bir merkez kelime ve içerik kelimesi ikilileriyle yapılır. Tekrardan kayıp fonksiyonuna bakalım. $$ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0 } \\log P(w_{t + j} \\mid w_t, \\theta) $$ Merkezi kelime $w_t$ için, kayıp fonksiyonu ayrı bir terimi her bir içerik kelimesi (w_{t + j}) (sliding window içerisindeki) (J_{t,j}(\\theta) = -\\log P(w_{t + j} \\mid w_t, \\theta\\ Bir örnek vererek bu durumu daha iyi anlayalım. Şu cümleyi ele alalım. Bugün bahçede bir top gördüm. Yeşil renkli bir kelimesi burada bizim merkezi kelimemizdir. Her seferinde bir kelimeye bakacağımız için, bir tane içerik kelimesi seçeceğiz. Örnek olarak top kelimesini ele alalım. Bundan sonra bu iki kelime için kayıp fonksiyonu $$ J_{t,j}(\\theta) = -\\log P(top \\mid bir) = -log \\frac{exp(u^{T}_{top} v_{bir})}{\\sum_{w \\ in V} exp(u^{T}_{w} v_{bir})} = -u_{top}^T v_{bir} + log \\sum_{w \\in V} exp(u^{T}_{w} v_{bir}) $$ Buradaki $V$ kümesi sliding windowu kapsayan kelimelerden oluşur. Loss (kayıp) fonksiyonumuzu aldığıma göre, şimdi vektörler üzerinde güncelleme yapalım. Burada hangi parameterlerin olduğuna göz atalım. merkezi kelime vektörlerinden sadece $v_{bir}$ içerik kelime vektörlerinden ise sliding window içerisindeki bütün kelimeler $u_w \\forall w \\in V$ Şuanki adımda sadece bu parametreler güncellenecek. $$ v_{bir} := v_{bir} - \\alpha \\frac{\\partial J_{t, j}(\\theta)}{\\partial v_{bir}} $$ $$ u_w = u_w - \\alpha \\frac{\\partial J_{t, j}(\\theta)}{\\partial u_{w}} \\forall w \\in V $$ Kayıp fonksiyonunu azaltacak şekilde yaptığımız her bir güncelleme, parametreler arasındaki benzerliği $v_{bir} \\hspace{1mm} ve \\hspace{1mm} u_{top}$ dot product’ını artırıyor ve aynı zamanda diğer her bir diğer $u_w$ ile $v_{bir}$ arasındaki benzerliği de azaltıyor. Bu biraz garip gelebilir ancak neden bir kelimesinin top kelimesinden hariç diğer kelimelerle benzerliğini azaltmaya çalışıyoruz. Diğerleri de mantıklı, içerik verecek kelimeler olabilir. Ancak bu bir sorun değil! Biz bu güncellemeyi her kelime için tek tek yaptığımızdan dolayı, yani her kelime bir kez merkezi kelime olacak, vektörler üzerindeki bütün güncellemelerin ortalaması metin içeriğininin dağılımını öğrenecektir. Bu yazıda partial derivative kısımlarına girilmemiştir. Ancak ben genel olarak Word2Vec modelinin nasıl çalıştığını anlatabildiğimi düşünüyorum. Eğer denemek isterseniz partial derivative kısımlarını kendiniz deneyebilirsiniz. Diğer yazılarda görüşmek üzere. Eğer yazıyı beğendiyseniz paylaşmayı unutmayın ki diğer insanlar da yararlansın. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:3","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"REFERENCES https://lena-voita.github.io/nlp_course/word_embeddings.html ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:3:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Problem Tanımı Knapsack problemi bilgisayar biliminde çok meşhur bir problemdir. Bu problemdeki amaç verilen ağırlık ve değerlerle en fazla değer toplayacak şekilde verilen ağırlık limitini aşmadan hangi itemlerin seçileceğidir. Knapscak problemi bir yüzyıldan fazla bir süredir, 1897 e kadar çalışmalar vardır. İsmini matematikçi Tobias Dantzig adlı matematikçinin eski çalışmalarından alır. Buradaki problemimiz için birden fazla yöntem vardır. Biz dinamik programlama ile bu problemin nasıl hallediliğine bakacağız. Şimdi problemdeki input ve istenilen outputa bakalım ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:0","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"INPUT Maksimum ağırlık limiti W ve elimizdeki paket sayısı n Ağırlıkların bulunduğu w[i] ve buna eş değer olan değer v[i] ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:1","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"OUTPUT Maksimum değer Hangi paketlerin alındığı ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:2","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"İMPLEMENTASYON Bu problemi analiz ederken algoritmanın hangi değerlere bağlı olacağını bulmaktır. Buradaki algoritmamız 2 ayrı değişkene dayanır. Bunların birincisi kaç tane paket taşıyacağımız ve elimizde kalan ağırlık limiti. Evet algoritmamızı iki değişkene bağlı şekilde yazacağız. Örnek olarak ilk 3 elemanı alarak, j maksimum limitli bir prpblemde optimum değer kaçtır. Buradaki ilk 3 eleman, hangi elemanları seçeceğimiz değişkenine örnektir. J limit ise ne kadar ağırlık limitimizin olduğudur. Bundan dolayı bir matrix oluşturup, her alt problemdeki optimum çözümü yazarsak bu şekilde istenilen sonuca ulaşabiliriz. Bundan dolayı [n+1][W+1] boyutlarında bir matrixte elimizdeki her alt alt problem için çözümleri saklayacağız. K[i][j] deki değer şu anlama geliyor: İlk i elemanı alarak j ağırlık limitli bir problemdeki optimum çözüm nedir. Peki bizim soruda ne isteniyordu? n elemanı kullanarak W limitli bir problemdeki çözüm nedir. Bundan dolayı bizim istediğimiz sonuç ise matrixin en son elemanı olan K[n][W] dir. Peki asıl soru olan her bu K[i][j] nasıl bulacağız. Öncelikle matriximizin ilk satırının hepsi 0 olacak. Bunun nedeni ise 0 item kullanırsak elde edebileceğimiz maksimum değer 0 dır. Diğer satırlarda ise durum farklıdır. Bundan dolayı her $1\\leq i \\leq n$ ve her $0 \\leq j \\leq W$ için bir durumu kontrol etmemiz gerekiyor. Kontrol etmemiz gereken değişken şuanki durumda yani item i ağırlığı şuanki j (ağırlık limiti) büyük mü. Çünkü eğer bizim şuanki itemimizin ağırlığı ağırlık limitimizden büyükse o itemi basitçe alamayız. Bu itemi alamadığımız için K[i][j] == K[i-1][j] olacak. Nedeni ise bu itemi seçmediğimiz için o durumdaki en optimum çözüm, o iteme kadar olanki en optimum çözüme eşittir. if w[i] \u003e j: K[i][j] = K[i-1][j] Eğer bu durum gerçekleşme ise elimizde iki seçenek var. Birinci seçenek şuanki itemi almamak. Bu durum yukarda bahsettiğimizin aynısı yani K[i][j] = K[i-1][j] İkinci seçenek ise bu itemi almak. Bu durumda elimizde olan optimum çözüm şu anlama geliyor. Şuanki itemin değeri v[i] Bu itemi aldığımız için geriye kalan j - w[i] ağırlık limitli ve ilk i-1 item arasındaki optimum çözüm, yani K[i-1][j-w[i]] Bu durumda ise optimum çözüm şu anlama geliyor. K[i][j] = v[i] + K[i-1][j - w[i]] Elimizde iki seçenek var. Peki hangisini seçeceğiz. Çok basit, en yüksek olan hangisi ise bunu seçeceğiz. Yani K[i][j] = max(K[i])[j], v[i] + K[i-1][j - w[i]]) Eğer bir basit bir kod yazmak istersek for j in range(W+1) K[0][j] = 0 //Yani İlk satırı 0 yap for i in range(1, n+1) for j in range(0, W + 1) if w[i] \u003e j // Eğer şuanki ağırlığımız şuanki limitten büyükse K[i][j] = 0 else K[i][j] = max(K[i-1][j], v[i] + K[i-1][j - w[i]]) Bizim çözümümüz ise K[n][W] deki değerdir. BackTracking kısmını sonra ekleyeceğim. ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:3","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"NUMPY import numpy as np ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:0:0","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"NUMPY ARRAY Python’daki listelere çok benzerdir. Numoy arrayleri sadece aynı veri türüne sahip listeleri barındırabilir. Arrayler daha az hafızada yer kaplar. Array oluşturmak için yapmamız gereken a = np.array([1, 2, 3, 4]) type(a) numpy.ndarray Buna ek olarak da, arraylere tuple ekleyebiliriz. Örnek olarak a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(\"Birinci eleman {}\".format(a[0])) print(\"Birinci elemanın birinci elemanı {}\".format(a[0][0])) Birinci eleman [1 2 3] Birinci elemanın birinci elemanı 1 ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:0","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY ÖZELLİKLERİ Arrayin boyutlarını öğrenmek için yapmamız gereken **array.shape** yazmak olacaktır Arrayin rankini öğrenmek için yapmamız gereken **array.ndim** yazmak olacaktır Arrayin boyutunu öğrenmek için yapmamız gereken **array.size** yazmak olacaktır Arrayin barındırdığı veri tipini öğrenmek için yapmamız gereken **array.dtype** yazmak olacaktır print(\"Arrayin boyutları {}\".format(a.shape)) print(\"Arrayin ranki {}\".format(a.ndim)) print(\"Arraydeki eleman sayısı {}\".format(a.size)) print(\"Arrayin veri tipi {}\".format(a.dtype)) Arrayin boyutları (3, 3) Arrayin ranki 2 Arraydeki eleman sayısı 9 Arrayin veri tipi int64 ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:1","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY FONKSİYONLARI Arraylerden Sıfır Oluşturmak için yapmamız gereken shape yerine istediğimiz boyutları girmek. Numpy bizim için gerekli arrayi oluşturacaktır. shape = (2, 2) zeros = np.zeros(shape) zeros array([[0., 0.], [0., 0.]]) Arraylerden Bir Oluşturmak için yapmamız gereken shape yerine istediğimiz boyutları girmek. Numpy bizim için gerekli arrayi oluşturacaktır. shape = (2, 2) ones = np.ones(shape) ones array([[1., 1.], [1., 1.]]) İstediğimiz bir değerle istediğimiz boyutta bir array oluşturmak için ise np.full kullanıyoruz. a = np.full((6,5), 29) print(a) [[29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29]] İdentity Matrix (Birim Matris) oluşturmak için ise np.eyeyazmak olacaktır. np.eye(3) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) Eğer belirli bir aralıkta belirli sayılarla artan bir array oluşturmak istiyorsak np.arange kullanmalıyız. rangearray = np.arange(10,100,10, dtype=float) rangearray array([10., 20., 30., 40., 50., 60., 70., 80., 90.]) Eğer yine belirli bir aralıkta değerler oluşturmak istiyorsak ve kaç tane oluşturacağımızı biliyorsak np.linspace kullanabiliriz. linarray = np.linspace(10, 100, 5) linarray array([ 10. , 32.5, 55. , 77.5, 100. ]) np.arange de 100 dahil değildi. Ancak np.linspace de dahil. Bunu da gözden kaçırmamak gerekir. Şimdi ise çok önemli bir fonksiyon olan np.reshape fonksiyonuna bakalım. Bu fonksiyon ile arraylerimizi istediğimiz formata çevirme şansımız var. array = np.arange(20) print(\"Önceki hali : \\n\", array) new_array = np.reshape(array, (4,5)) print(\"Sonraki hali : \\n\", new_array) Önceki hali : [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] Sonraki hali : [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:2","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY INDEKSLEME (ARRAY INDEXING) Numpy arrayleri indekslemede çok kolaylık sağlıyor. Bir Boyutlu Array a1 = np.array([1, 3, 4, 5, 2, 10]) a1[0] 1 a1[4] 2 a1[-1] 10 a1[-3] 5 ÇOK BOYUTLU ARRAY a2 = np.array([[3, 4, 5, 6], [1, 3, 7, 2], [8, 4, 5, 10], [12, 124, 125, 126]]) a2[0] array([3, 4, 5, 6]) a2[0, 0] 3 a2[2, -1] # 2.indexin son elemanı 10 a2[2, 0] = 100 # 2.elemanın 0.elemanını 100 yap a2 array([[ 3, 4, 5, 6], [ 1, 3, 7, 2], [100, 4, 5, 10], [ 12, 124, 125, 126]]) a2[[0, 0, 2, 1]] # 0.index, 0.index, 2.index, 1.index array([[ 3, 4, 5, 6], [ 3, 4, 5, 6], [100, 4, 5, 10], [ 1, 3, 7, 2]]) a2[:2, ::2] # 2.satıra kadar 0 ile 2. indexler array([[3, 5], [1, 7]]) a2[::-1, ::-1] # Arrayi ters çevir array([[126, 125, 124, 12], [ 10, 5, 4, 100], [ 2, 7, 3, 1], [ 6, 5, 4, 3]]) a2[:, 0] # İlk sutün array([ 3, 1, 100, 12]) a2[0, :] # İlk satır array([3, 4, 5, 6]) I will ad other features to see how it is going ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:3","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"Merhaba bu yazımızda Makine Öğrenmesinde meşhur bir algoritma olan Knn algoritmasını sıfırdan yazacağız. Tabii ki hazır bir sürü kütüphane var ancak sıfırdan algoritmayı yazabilmek bize algoritmanın nasıl çalışacağını gösterecektir. Böylece Knn algoritması bir tahmin yaparken nasıl yapıyor olayın arkasında neler dönüyor bunları anlayabiliyor olacağız. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:0:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"K-Nearest Neighbour Nedir Öncelikle şunu bilmek gerekir ki K-Nearest-Neighbour adından da anlaşılacağı üzere en yakın k komşu noktalara bakıp en çok hangi label varsa o labelı tahmin(prediction) olarak verir. Peki bu yakınlık uzaklık ilişkisi nasıl kurulur önce ona bakalım. Uzaklığı ölçebilmek için belli başlı algoritmalar vardır. Bunlardan biri eucledian diğeri de manhattan uzaklığıdır. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Eucledian Uzaklığı Manhattan uzaklığında aslında iki nokta arasında uzaklığı alırken normal 2 boyutlu denklemde nasıl alıyorsak, bunun n boyutlu formüle döndürülmüş halidir. Örnek olarak $a = (x_1, y_1)$ ve $b = (x_2, y_2)$ olsun. Bu noktalar arasında uzaklığı bulurken yaptığımız işlem $$d(a, b) = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$ Peki eğer bizim verimiz n boyutlu olursa bu uzaklık nasıl ölçülecek?. Bu durumda ise uzaklık $$d(a,b)= \\sum_{i=1}^n (a_i - b_i)^2$$ Bu formulu ise Numpy ile şu şekilde yazabiliriz np.sqrt(np.sum(np.square(a - b), axis=1)) ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:1","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Manhattan Uzaklığı Manhattan uzaklığında iki nokta arasındaki uzaklık her bir alt noktanın farkının mutlak değerlerinin toplamı ile bulunur. Örnek olarak $a = (x_1, y_1)$ ve $b = (x_2, y_2)$ olsun. Bu noktalar arasında uzaklığı bulurken yaptığımız işlem $$d(a, b) = \\lvert x_1 - x_2\\rvert + \\lvert y_1 - y_2 \\rvert$$ Peki eğer bizim verimiz n boyutlu olursa bu uzaklık nasıl ölçülecek?. Bu durumda ise uzaklık $$d(a,b)= \\sum_{i=1}^n \\lvert a_i - b_i\\rvert$$ Bu formulu ise Numpy ile şu şekilde yazabiliriz np.sum(np.abs(a - b), axis=1) ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:2","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Algoritma Akışı KNN algoritmasında eğitme (training) işlemi aslında sadece verilen veriyi ezberlemekten ibarettir. Bundan dolayı eğitme kısmında bir şey yapmayacağız ancak tahmin etme (prediction) kısmında ise asıl üstteki formuülleri kullanıp işlem yapacağız. Bu algoritmayı Python ile Numpy Kullanarak implement edeceğiz. Önceklikle şu komut ile Numpy kütüphanesini import edelim. import numpy as np Şimdi Bir tane class tanımlayalım. Öncelikle kaç tane komşu kullanacağı modelin bir parametresi k olacak. Daha sonra hangi uzaklık formülünü kullanacağı da modelin bir parametresi olacak. Hadi başlayalım. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık Üstteki kod bloğunda yaptığımız işlem aslında modelin parametrelerini constructor fonskiyonunda tanımlamak oldu. Şimdi modelin eğitme fonksiyonunu yazalım. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık def fit(self, X, y): self.X = X self.y = y Yukarıda da bahsettiğimiz gibi Knn algoritması aslında sadece eğitme verisini ezberler. Bütün işlemler prediction kısmında yapılır. Bundan dolayı eğitme verisini modelin eğitim seti olarak değiştirebiliriz. Şimdi en önemli konu olan prediction kısmına gelelim. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:2:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Tahmin Etme (Prediction) Kısmı Nasıl Olacak? Öncelikle uzaklıklar hesaplanacak Daha sonra en yakın k tane nokta alınacak Daha sonra bu en yakın k noktanın label sayılarını belirleyeceğiz. Yani hangi labeldan kaç tane olduğunu belirleyeceğiz. Daha sonra en çok sayısı olan label bizim tahminimiz olarak Diyelim ki bizim elimizde bir tane a verisi olsun ve eucledian uzaklık kullanıyor olalım. Bir nokta için nasıl bir işlem gerekiyor ona bakalım. #Uzaklık Hesaplama uzaklıklar = np.sqrt(np.sum(np.square(X - a), axis=1)) #En yakın k noktanın indexlerini bulalım en_yakın_k_index = np.argsort(uzaklıklar)[:k] #Şimdi bu en yakın k indexin hangi labellara ait olduğunu bulalım. en_yakın_labellar = y[en_yakın_k_index] #Daha sonra bu en yakın labellarda her labeldan kaç tane olduğunu bulalım labellar, adetler = np.unique(en_yakın_labellar, return_counts=True) #Daha sonra en çok hangi labelın sayısı var bunu bulalım max_label_index = np.argmax(adetler) #Daha sonra en çok sayısı olan label döndürelim return labellar[max_label_index] Şimdi burada bir sürü numpy fonksiyonu kullandık, bunlar kafa karıştırmış olabilir. Bundan dolayı bu fonksiyonlar ne işe yarıyor kısaca anlatayım. np.argsort(array): Bu fonksiyon parametre olarak aldığı arrayi sortlayacak indexleri verir. Aslında arrayi sortlamaz, ancak hangi indexler arrayi sortlar onu verir. np.unique(array, return_counts=True): Bu fonksiyon ise array içerisindeki unique elemanları dönderir. Eğer return_counts=True ise o zaman bu unique elemanlardan kaç tane var onu da gösterir. Örnek olarak array = [2, 3, 4, 3, 2, 10, 2] labellar, sayılar = np.unique(array, return_counts) print(labellar) #(2, 3, 4, 10) print(sayılar) #(3, 2, 1, 1) Labellar bizim arrayimizde hangi unique label var onları dönderir. Sayılar ise hangi labeldan kaç tane var onu dönderir. Örnek olarak 2 den 3 tane var. 10’dan 1 tane var. np.argmax(array): Bu fonksiyon array içerisindeki maximum elemanın indexini verir. Mesela yukarıdaki arrayde maximum eleman 10 ve 10’un indexi 5 dir. np.argmax() bu 5 indexini dönderir. Şimdi bu bir nokta içindi. Bunu test verimizde her bir nokta için yapalım. KNN Classının içerisinde implement edelim. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık def fit(self, X, y): self.X = X self.y = y def predict(self, X_test): predictions = [] for point in X_test: if self.uzaklık == \"eucledian\": uzaklık = np.sqrt(np.sum(np.square(self.X - point), axis=1)) elif self.uzaklık == \"manhattan\": uzaklık = np.sum(np.abs(self.X - point), axis=1) indices = np.argsort(uzaklık)[:self.k] near_labels = self.y[indices] labels, values = np.unique(near_labels, return_counts=True) max_ind_label = np.argmax(values) prediction = labels[max_ind_label] predictions.append(prediction) return np.array(predictions) Yaptığımız işlemler her nokta için uzaklığı hesapladık en yakın k noktanın labellarını aldık bu labelların sayılarını öğrendik en çok label kimdeyse onu tahmin olarak öne sürdük Bu KNN modelini istediğiniz veride kullabilirsiniz. Daha kapsamlı koda bakmak isterseniz bu Github Repo’ya bakabilirsiniz. Bir sonraki yazıda görüşmek üzere. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:2:1","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"TANIM Softmax fonksiyonu modelden çıkan sonuçların olasılıksal şekilde ifade edilmesi için kullanılan bir fonksiyondur. Genellikle nöral ağlarda (neural network) ağın sonucunu sınıflara olasılık değerleri vermek için kullanılır. Softmax fonksiyonu input olarak $K$ boyutlu uzaydan vektör $z$ alır. Bu vektörü $K$ olasılık değerlerinden oluşan bir olasılık dağılımına çevirir. Bu olasılıkların her biri exponentialları ile doğru orantılıdır. Softmax fonksiyonu uygulamadan önce bu $z$ vektöründeki bazı değerler negatif de olabilir 0 da olabilir, pozitif de olabilir. Softmax fonksiyonunu uyguladıktan sonra ise bütün değerler $(0, 1)$ aralığında değer alır ve bütün değerlerin toplamı 1 olur. Standart softmax function tanımı şu şekildedir. $\\sigma : \\mathbb{R^{K}} \\rightarrow \\mathbb{R^K}$ $$ \\sigma(z)_{i} = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} her \\hspace{1mm} i = 1, 2, 3, ..., K ve \\hspace{1mm} z = (z_1, z_2, ... , z_k) \\in \\mathbb{R^K} $$ Bir diğer deyişle bizim yaptığımız işlem her bir değerin exponential fonksiyonunu almak ve bunu toplama bölmek. Böylece normalize etmiş oluyoruz ve bütün değerleri topladığımız zaman sonuç 1 ediyor. Örnek olarak vektör $k = [1, 1, 1] \\in \\mathbb{R^3}$ olsun. O zaman, $$ \\sigma(k) = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}] $$ Peki bunu nasıl bulduk. Öncelikle toplamı hesaplayalım $$ \\sum_{j=1}^{K}e^{k_i} = e^1 + e^1 + e^1 = 3e $$ Toplam $3e$ çıktı. Şimdi her bir değeri exponential fonskiyona input olarak verirsek çıkacak sonuç $e^1 = e$ olur. Bizim softmax fonksiyonunda yaptığımız işlem ise bu değerleri alıp toplama bölmek. Yani, $$ \\frac{e}{3e} = \\frac{1}{3} $$ ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:1:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"NUMPY İLE NASIL İMPLEMENT EDİLİR Numpy fonksiyonunda arrayin direk exponential fonksiyonunu alabiliriz. Bunun için for loop açmamıza gerek yok import numpy as np arr = np.array([1, 3, 2]) exponential_arr = np.exp(arr) print(\"Array: {} \\nExponential Array: {} \\n\".format(arr, exponential_arr)) Array: [1 2 3] Exponential Array : [ 2.71828183 7.3890561 20.08553692] Arrayin direk üstel şekilde toplamını da alabiliriz. sum_of_exponentials = np.sum(exponential_arr) print(\"Exponential Array Toplamı: \", sum_of_exponentials) Exponential Array Toplamı: 30.19287485057736 Şimdi softmax implement etmek için her şeye sahibiz. Fonksiyon şeklinde implement edebiliriz. def softmax(arr): exp_array = np.exp(arr) exp_toplam = np.sum(exp_array) return exp_array/exp_toplam Şimdi fonksiyonumuzu test edelim arr = np.array([1, 1, 1]) softmax_array = softmax(arr) print(\"Array: {} \\nSoftmax Array: {}\".format(arr, softmax_array)) Array: [1 1 1] Softmax Array: [0.33333333 0.33333333 0.33333333] Gördüğümüz üzere softmax fonksiyondan çıkan arrayin toplamı 1 e eşit oluyor np.sum(softmax_array) #Sonuç 1 çıkıyor. Softmax fonksiyonu bu kadar. Bir sonraki yazıda görüşmek üzere. ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:2:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"REFERENCES wikipedia-softmax ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:3:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"TANIM İstatistik, verilen bir örneklemden(sample) elde edilen herhangi bir değer demektir. İstatistiksel öğrenmede, verilen sampledan sağlanan bilgi ile karar verilir. İlk yaklaşımımız, sample’ın belirli bir dağılımdan (distribution) geldiğini farz ederek yapmak olacaktır. Bu dağılıma örnek olarak Gaussian dağılım verilebilir. Bu durumun avantajı ise, parametre sayısının azaltılması olacaktır. Tüm parametrelerimiz ortalama değer (mean) ve varyans (variance) olacaktır. Bu parametreleri sample tarafından elde ettikten sonra, bütün dağılımı biliyor olacağız. Bu parametreleri verilen sample üzerinden öğrenip, daha sonra bu bulduğumuz ortalama ve varyans değerlerini modele entegre ederek, tahmini bir dağılım elde edeceğiz. Daha sonra bu dağılımı da karar vermek için kullanacağız. Öncelikle olasılık kavramı diğer ismiyle density estimation (yoğunluk tahmini) anlamına gelen $p\\left(x\\right)$ kavramı ile başlıyoruz. Bu kavramı, Naive Bayesde de olduğu gibi tahmini olasılıkların $p(x \\mid C_{i})$, ve prior olasılık olan $P\\left(C_{i}\\right)$ olduğu ve bu olasılıkların daha sonra asıl amaç olan $P\\left(C_{i} \\mid x\\right)$‘i tahmin ederek sınıflandırma işlemi yapılması için kullanıyoruz. Peki bu parametreleri nasıl öğreneceğiz. Maksimum Likelihood Estimation kullanarak yapacağız. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:0","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"},{"categories":null,"content":"Maximum Likelihood Estimation (Maksimum Olasılık Tahmini) Elimizde birbirinden bağımsız ve aynı şekilde dağıtılmış olan bir sample var. Bu sample’ı $X = \\{ x^{t} \\}_{i=1}^{N}$ şeklinde gösterebiliriz. Bu sampledan çekilen her bir $x^{t}$ örneğin, bilinen bir olasılık dağılımına ait olduğunu varsayıyoruz. Bu olasılık dağılımını da $p\\left(x \\mid \\theta \\right)$ gösteriyoruz. $$ x^{t} \\sim p(x|\\theta) $$ Bizim buradaki amacımız bize en yüksek olasılığı $p\\left(x \\mid \\theta \\right)$ verecek olan $\\theta$ değerini bulmak. Bütün örnekler $x^{t}$ birbirinden bağımsız olduğundan parametre $\\theta$ nın olasılık fonksiyonu bütün verilen sampleların olasılıklarının çarpımına eşittir. $$ l(\\theta | X) = p(X|\\theta) = \\prod_{t=1}^{N}p(x^{i}|\\theta) $$ Maksimum olasılık tahmininde, bu değeri maksimum yapan $\\theta$ değerini bulmak istiyoruz. Bunu bulmak için önce logaritma alıp, daha sonra nerede maksimum yaptığına bakabiliriz. Logaritma alma sebebimiz ise logaritmanın çarpım sembolünü toplama çevirmesi ve başka kolaylıklar sağlaması dolayısıyladır. Log olasılık ise şöyle tanımlanır. $$ l(\\theta | X) \\equiv \\log l(\\theta | X) = \\sum_{t = 1}^{N}\\log p(x^{t}|\\theta) $$ Yazımızın başında bu her sample ın belirli bir dağılımdan geldiğini söylemiştik. Bunun için bir sürü seçenek olabilir. Bernouilli, Multinomial ve Gaussian(Normal) dağılımlar olabilir. Ancak biz burada sadece Gaussian(Normal) dağılım ile ilgilineceğiz. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:1","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"},{"categories":null,"content":"NORMAL DAĞILIMDA MAXİMUM LIKELIHOOD ESTIMATION X, ortalama yani $E[X] \\equiv \\mu$ ve varyans $Var(X) \\equiv \\sigma^{2}$ değerlerine sahip normal dağılımla dağıtılmış bir random variable olsun. O zaman density (yoğunluk) fonksiyonu şu şekilde $$ N(\\mu , \\sigma^{2}) = p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^{2}}} $$ O zaman verilen sampleın $X = \\{ x^t \\}_{t=1}^{N}$ log likelihood değeri de şu şekilde olur. $$ l(\\mu, \\sigma | X) = -\\frac{N}{2}\\log(2\\pi) - N \\log(\\sigma) - \\frac{\\sum_{t}(x^t - \\mu)^{2}}{2\\sigma^{2}} $$ Daha sonra sırayla bu fonksiyonun ortalama değer ve varyans için partial türevlerini alıp sıfıra eşitlediğimizde ortaya şöyle bir sonuç çıkıyor. $$ m = \\frac{\\sum_{t}x^t}{N} $$ $$ s^2 = \\frac{\\sum_{t}(x^t - m)^2}{N} $$ Burada $m$ ortalama değer için maximum likelihood estimate oluyor ve $s^2$ ise varyans için maximum likelihood estimate oluyor. Bu durumda istenilen parametreleri bulmuş olduk. Bundan sonraki yazıda ise bias(önyargı) ve Varyans(Variance) konuları işleyeceğiz. Sonraki yazılarda görüşmek üzere. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:2","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"}]