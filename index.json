[{"categories":null,"content":"Bu yılın başında hedeflerimden birisi fazla yazı yazmaktı. Neden böyle bir karar aldığımı açıklayayım. Blog yazmaktan ziyade, yazma sürecini sevmem. blog yazabilmek için yazdığınız konuyu iyice öğrenmek gerekiyor. bir şeyleri öğrenmenin verdiği haz beni mutlu ediyor. Yazılarımın ne kadar az kişi tarafından olsa da okunduğunu görmek gerçekten keyif veriyor. Çevremdeki bazı insanların blogum hakkındaki pozitif yorumları. Yılın ilk yarısında ne kadar fazla blog yazmak gibi bir hedefim olsa da bunu tutturamadım. Ancak yılın ikinci yarısında, özellikle Ağustos ayından itibaren içimde gerçekten müthiş bir yazı yazma isteği var, umarım kaybolmaz. Bundan sonra yazmak istediğim konular Matematik Ekonomi Okuduğum bazı kitapların değerlendirmeleri Bir konuyu öğrenmenin en iyi yolunun o konuyu öğretmek olduğunu düşünüyorum, bunu üniversite zamanında asistanlık yaparken daha iyi anladım. Richard Feynman on Lecture İşin kötü yanı bu kadar hevesli olup bazı kişisel sebeplerden dolayı 1 ay gibi bir süre ara vermek zorunda olmam. Bu süreyi de genel olarak öğrendiklerim hakkında düşünerek geçirmeyi planlıyorum. Buradan blogum hakkında iyi veya kötü yorumları olan herkese sevgilerle, takipte kalınız. REFERENCES https://en.wikipedia.org/wiki/Richard_Feynman ","date":"2023-09-08","objectID":"/bundan-sonra-ne-yazacagim/:0:0","tags":["life"],"title":"(TR) Bundan Sonra Ne Yazacağım","uri":"/bundan-sonra-ne-yazacagim/"},{"categories":null,"content":"Introduction In the ever-evolving landscape of web development, GraphQL has emerged as a powerful alternative to traditional RESTful APIs. Its flexibility and efficiency have led many developers to consider migrating their REST endpoints to GraphQL. In this blog post, we will explore the process of converting a RESTful endpoint to GraphQL, unlocking the benefits of a more customizable and efficient data-fetching experience. Join us on this journey as we delve into the world of GraphQL and transform a RESTful API into a GraphQL powerhouse. ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:1:0","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"Rest API Implementation Let’s say that we have a server with a REST Endpoint structured as below. curl http://localhost:8080/artists and it returns [ { \"name\": \"The Weeknd\", \"age\": 30, \"tracks\": [ { \"name\": \"Creepin\", \"duration\": 222 } ] }, { \"name\": \"Tame Impala\", \"age\": 30, \"tracks\": [ { \"name\": \"Let It Happen\", \"duration\": 467 } ] } ] This endpoint simply can be handled by this below code. package main import ( \"encoding/json\" \"log\" \"net/http\" ) type Track struct { Name string `json:\"name\"` Duration int `json:\"duration\"` // in seconds } type Artist struct { Name string `json:\"name\"` Age int `json:\"age\"` Tracks []Track `json:\"tracks\"` } var data = []Artist{ { Name: \"The Weeknd\", Age: 30, Tracks: []Track{ {Name: \"Creepin\", Duration: 222}, }, }, { Name: \"Tame Impala\", Age: 35, Tracks: []Track{ {Name: \"Let It Happen\", Duration: 467}, }, }, } func main() { mux := http.NewServeMux() mux.HandleFunc(\"/artists\", func(w http.ResponseWriter, r *http.Request) { if err := json.NewEncoder(w).Encode(\u0026data); err != nil { return } }) log.Fatal(http.ListenAndServe(\":8080\", mux)) } ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:2:0","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"Structure Change But you only want some of the fields maybe like [ { \"name\": \"The Weeknd\", \"tracks\": [ { \"name\": \"Creepin\", } ] }, { \"name\": \"Tame Impala\", \"tracks\": [ { \"name\": \"Let It Happen\", } ] } ] You want to make it as a GraphQL query maybe something like query { getArtists { name tracks { name } } } Question is how to change this really simple API to a graphQL endpoint. ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:2:1","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"GraphQL Implementation For this we will use the package of https://github.com/99designs/gqlgen, you can have a look. To start the project, we will follow the quick quide. First create the project mkdir example cd example go mod init example Add 99designs/gqlgen to your project’s tools.go printf '// +build tools\\npackage tools\\nimport (_ \"github.com/99designs/gqlgen\"\\n _ \"github.com/99designs/gqlgen/graphql/introspection\")' | gofmt \u003e tools.go go mod tidy Initialise gqlgen config and generate models go run github.com/99designs/gqlgen init go mod tidy Start the graphql server go run server.go ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:3:0","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"Project structure Your folder structure should look like this ├── go.mod ├── go.sum ├── gqlgen.yml ├── graph │ ├── generated.go │ ├── model │ │ └── models_gen.go │ ├── resolver.go │ ├── schema.graphqls │ └── schema.resolvers.go ├── server.go └── tools.go ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:3:1","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"Generated Code and GraphiQL Playground Your server.go will look like this package main import ( \"log\" \"net/http\" \"os\" \"github.com/99designs/gqlgen/graphql/handler\" \"github.com/99designs/gqlgen/graphql/playground\" \"github.com/ocakhasan/graph/graph\" ) const defaultPort = \"8080\" func main() { port := os.Getenv(\"PORT\") if port == \"\" { port = defaultPort } srv := handler.NewDefaultServer(graph.NewExecutableSchema(graph.Config{Resolvers: \u0026graph.Resolver{}})) http.Handle(\"/\", playground.Handler(\"GraphQL playground\", \"/query\")) http.Handle(\"/query\", srv) log.Printf(\"connect to http://localhost:%s/ for GraphQL playground\", port) log.Fatal(http.ListenAndServe(\":\"+port, nil)) } When you run all of the commands above and run the project you should see something like this on your project We will be testing our query from the UI to easily see the results. ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:3:2","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"GraphQL File There is an autogenerated file schema.graphqls, we will be setting our Artist and Track models here. type Artist { name: String! age: Int! tracks: [Track!]! } type Track { name: String! duration: Int! } type Query { artists: [Artist!]! } Then run below command to auto-generate models resolver etc, we will just fill the logic. go run github.com/99designs/gqlgen generate Then the schema.resolvers.go file will be like this package graph // This file will be automatically regenerated based on the schema, any resolver implementations // will be copied through when generating and any unknown code will be moved to the end. // Code generated by github.com/99designs/gqlgen version v0.17.36 import ( \"context\" \"fmt\" \"github.com/ocakhasan/graph/graph/model\" ) // Artists is the resolver for the artists field. func (r *queryResolver) Artists(ctx context.Context) ([]*model.Artist, error) { panic(fmt.Errorf(\"not implemented: Artists - artists\")) } // Query returns QueryResolver implementation. func (r *Resolver) Query() QueryResolver { return \u0026queryResolver{r} } type queryResolver struct{ *Resolver } We will fill the Artists method, it will be a simple returning array of model.Artist which is in the model/models_gen.go (auto-generated file) // Code generated by github.com/99designs/gqlgen, DO NOT EDIT. package model type Artist struct { Name string `json:\"name\"` Age int `json:\"age\"` Tracks []*Track `json:\"tracks\"` } type Track struct { Name string `json:\"name\"` Duration int `json:\"duration\"` } ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:3:3","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"Implementation of the Resolver To implement the resolver we will return a hardcoded array of model.Artist struct. package graph // This file will be automatically regenerated based on the schema, any resolver implementations // will be copied through when generating and any unknown code will be moved to the end. // Code generated by github.com/99designs/gqlgen version v0.17.36 import ( \"context\" \"github.com/ocakhasan/graph/graph/model\" ) var data = []*model.Artist{ { Name: \"The Weeknd\", Age: 30, Tracks: []*model.Track{ {Name: \"Creepin\", Duration: 222}, }, }, { Name: \"Tame Impala\", Age: 60, Tracks: []*model.Track{ {Name: \"Let It Happen\", Duration: 467}, }, }, } // Artists is the resolver for the artists field. func (r *queryResolver) Artists(ctx context.Context) ([]*model.Artist, error) { return data, nil } // Query returns QueryResolver implementation. func (r *Resolver) Query() QueryResolver { return \u0026queryResolver{r} } type queryResolver struct{ *Resolver } Let’s run the server again and go to GraphiQL playground (http://localhost:8080/). go run server.go Then go to http://localhost:8080 and paste the query query { artists { name } } it will return { \"data\": { \"artists\": [ { \"name\": \"The Weeknd\" }, { \"name\": \"Tame Impala\" } ] } } You can play with the editor and convert your rest endpoints to GraphQL easily. ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:3:4","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"REFERENCES In summary, transitioning from REST to GraphQL offers numerous benefits for your API. It’s a journey worth taking, promising improved efficiency and flexibility. So, take that step, and may your GraphQL journey be both rewarding and transformative. Happy coding! https://github.com/99designs/gqlgen ","date":"2023-09-07","objectID":"/golang-setup-graphql-server/:4:0","tags":["golang","network","rest","graphql"],"title":"Setting up a GraphQL Server in Golang","uri":"/golang-setup-graphql-server/"},{"categories":null,"content":"MOTIVATION I work as a Backend engineer almost 2 years now as of August 2023. My work mostly relies on database systems such as MySQL, Redis, Mongo etc. So it would be great to learn the internals or system designs related to those database systems. Also it is stated in the book that, aynone working on backend side who processes data and the applications they developed uses internet should read this book, so I am quite a fit for the people who should read this book. I will make a blog post on each of the chapter I read, mostly I will read after my working hours so it will probably take months for me to really finish this book. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:0:0","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"CHAPTER 1 - Reliable, Scalable and Maintainable Applications Most applications are data-intensive nowadays, the problems mostly related to amount of data etc. Most of the tools developed are highly advanced nowadays but none of them can meet all of the needs of different data processing and storing requirements. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:1:0","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"Definitions Reliability The system should continue to work correctly even though a system error occurs. tolerate human errors prevents unauthorized access there could be some hardware problems such as hard disk crashs, ram becomes faulty etc. design systems in a way that human errors opportunity are minimized. test your system, froom unit to integration tests. setup monitoring tools, perfomance metrics and error rates. Scalability The system should handle the load gracefully if the volume (data, network etc) grows. what happens to system resources when you increase the load to your system how much resource you need to increase when you increase the load. response time is what client sees, request sent and response is received from the client latency is the duration that a request is waiting to be handled in response times it is better to use percentile, not the average. because it does not tell you how many users are affected by a specific number of delay. Maintainability Project should be easily developed by many other engineers who work on the project. cost of software are mostly based on the ongoing mainteiance, not the initial software development. projects should be evolvable: meaning making changes should be easy. simple: a project should not be complex, should be easy to work with. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:1:1","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"CHAPTER 2 - Data Models and Query Languages ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:2:0","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"Relational Vs Document Model Most famous data format is SQL. Goal of relational model was to hide the implementation detail behind a cleaner interface rather than forcing developers to think the internal representation of the data. The driving forces for NoSQL (Document) Databases need for greater scalability specialized query operations not supported by SQL more dynamic and expressive data models. Chapter 2 will be continued. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:2:1","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"CHAPTER 3 - STORAGE AND RETRIEVAL On the most basic model, a database needs to do 2 operations. it should store the given data when ask it again later, it should give the data back. The questions needs to be asked as an application developer probably would not be how the database handles storage and retrieval internally? But if you have to tune the program you use, it is better to know the internals of the tool. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:3:0","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"WORLD SIMPLEST DATABASE Would be a key value store written into a file. db_set () { echo \"$1,$2\" \u003e\u003e database } db_get () { grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1 } Similarly to what db_set function does, the databasess also uses a log internally, append-only data file. db_get function performance is terrible on large scale of data since it traverse the all of the file O(N). ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:3:1","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"Index To retrieve the data efficiently, you need an index. Index is an additional data which can be derived from the original set of data. Creating indexes may create an overhead to write operations, since it cannot be more efficient than writing to end of file. ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:3:2","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"HASH INDEXES ","date":"2023-08-27","objectID":"/notes-on-designing-data-intensive-applications/:3:3","tags":["backend","database","books"],"title":"My Notes on Designing Data Intensive Applications","uri":"/notes-on-designing-data-intensive-applications/"},{"categories":null,"content":"Introduction Before going any further all of the code can be found in the local-go-sqs-setup. Welcome to our blog post on using local Amazon Simple Queue Service (SQS) with Golang! In this guide, we’ll explore how to harness the power of local SQS within your Golang applications. By setting up and running SQS on your local environment, you can achieve seamless development and testing, reducing cloud dependencies and potential costs. Let’s dive in and discover how to supercharge your Golang projects with this efficient, on-premises queuing solution! ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:0:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"Technologies we use Docker We will use the docker image of the softwaremill/elasticmq for the local SQS Golang softwaremill/elasticmq is a tool which runs a SQS compatible server on your local environment. ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:1:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"Setup the Local SQS To be able to run an sqs server locally, simply run the command docker run -p 9324:9324 -p 9325:9325 softwaremill/elasticmq Then, go to your browser and enter the url of http://localhost:9325. You will see something like this. ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:2:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"Create A Queue To create a queue, run the command aws sqs create-queue --endpoint-url http://localhost:9324 --queue-name test_queue --region eu-west-1 the response will be something like this. { \"QueueUrl\": \"http://localhost:9324/000000000000/test_queue\" } And the browser will be ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:3:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"How To Integrate With Go Normally that’s how you create a sqs client in go to list the queue urls. package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" \"github.com/aws/aws-sdk-go-v2/service/sqs\" ) func main() { cfg, err := config.LoadDefaultConfig(context.Background()) if err != nil { log.Fatal(err) } sqsClient := sqs.NewFromConfig(cfg) res, err := sqsClient.ListQueues(context.Background(), \u0026sqs.ListQueuesInput{ MaxResults: aws.Int32(10), NextToken: nil, QueueNamePrefix: nil, }) if err != nil { log.Printf(\"error while listing the queues\") } for _, queue := range res.QueueUrls { log.Println(queue) } } However, to be able to connect to local queue we need an EndpointResolverWithOptions which will redirect the requests to http://localhost:9324 type EndpointResolverWithOptions interface { ResolveEndpoint(service, region string, options ...interface{}) (Endpoint, error) } To be able to do it, we can create a simple struct which implements the EndpointResolverWithOptions interface. type localResolver struct{} func (l localResolver) ResolveEndpoint(service, region string, options ...interface{}) (aws.Endpoint, error) { return aws.Endpoint{ URL: \"http://localhost:9324\", SigningRegion: \"eu-west-1\", }, nil } The all of the code is package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" \"github.com/aws/aws-sdk-go-v2/service/sqs\" ) type localResolver struct{} func (l localResolver) ResolveEndpoint(service, region string, options ...interface{}) (aws.Endpoint, error) { return aws.Endpoint{ URL: \"http://localhost:9324\", SigningRegion: \"eu-west-1\", }, nil } func main() { cfg, err := config.LoadDefaultConfig(context.Background()) if err != nil { log.Fatal(err) } cfg.EndpointResolverWithOptions = localResolver{} sqsClient := sqs.NewFromConfig(cfg) res, err := sqsClient.ListQueues(context.Background(), \u0026sqs.ListQueuesInput{ MaxResults: aws.Int32(10), NextToken: nil, QueueNamePrefix: nil, }) if err != nil { log.Printf(\"error while listing the queues\") } for _, queue := range res.QueueUrls { log.Println(queue) } } When you run this code with the command go run main.go You will see something like 2023/07/22 20:27:20 http://localhost:9324/000000000000/test_queue In conclusion, leveraging local Amazon SQS with Golang empowers you to create efficient message queuing systems in your development environment. By reducing cloud dependencies and streamlining development and testing, you’ll save valuable time and resources. Embrace SQS for scalable, decoupled, and reliable systems, and keep exploring its possibilities for your specific use cases. Happy coding! If you have questions, please reach out to me. ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:4:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"REFERENCES https://github.com/softwaremill/elasticmq https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/sqs Thanks for reading, please let me know if you have any questions. ","date":"2023-07-22","objectID":"/use-sqs-locally-with-golang/:5:0","tags":["sqs","go","consumer"],"title":"Local SQS Setup With Golang","uri":"/use-sqs-locally-with-golang/"},{"categories":null,"content":"Motivation The motivation for me to write this blog post is that I want to have a consumer which uses goroutines for the messages received from SQS but almost all of the posts I read was did not exactly implemented as a worker pool integration. The posts uses new goroutines for each of the messages received and it might be useful for their case but if you process millions of records, creating and deleting millions of records might be a burden to garbage collector. So in this post, 10 goroutines will listen for all of the messages received. Note This case only will work if you need to process and delete the messages from the queue in each of the ReceiveMessage call. Otherwise, it might not be useful for your case. ","date":"2023-07-07","objectID":"/golang-sqs-consumer-worker-pool/:1:0","tags":null,"title":"Golang Sqs Consumer Worker Pool","uri":"/golang-sqs-consumer-worker-pool/"},{"categories":null,"content":"Design ","date":"2023-07-07","objectID":"/golang-sqs-consumer-worker-pool/:1:1","tags":null,"title":"Golang Sqs Consumer Worker Pool","uri":"/golang-sqs-consumer-worker-pool/"},{"categories":null,"content":"Implementation There are some things to consider. For our case the consumer should do following steps Receive message from the queue it can receive at most 10 messages in one single call to sqs. Send these 10 messages to the channel for workers to listen. Wait for these 10 messages process to finish. Release the workers so they can process again. It might be useful for your case, so please use with care with your judgement. ","date":"2023-07-07","objectID":"/golang-sqs-consumer-worker-pool/:2:0","tags":null,"title":"Golang Sqs Consumer Worker Pool","uri":"/golang-sqs-consumer-worker-pool/"},{"categories":null,"content":"CODE To understand the functions and methods used here, please have a visit to aws-sdk-go-v2/sqs import ( \"context\" \"sync\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/service/sqs\" \"github.com/aws/aws-sdk-go-v2/service/sqs/types\" ) type Consumer struct { client sqs.Client queueName string } func (consumer *Consumer) Start(ctx context.Context) { params := \u0026sqs.ReceiveMessageInput{ AttributeNames: []types.QueueAttributeName{types.QueueAttributeNameAll}, MaxNumberOfMessages: 10, // max it can receive MessageAttributeNames: []string{string(types.QueueAttributeNameAll)}, QueueUrl: aws.String(consumer.queueName), WaitTimeSeconds: 20, // wait for 20 seconds at max for at least 1 message to be received } msgCh := make(chan types.Message) var wg sync.WaitGroup startPool(ctx, msgCh, \u0026wg) for { select { case \u003c-ctx.Done(): close(msgCh) return default: resp, err := consumer.client.ReceiveMessage(ctx, params) if err != nil { log.Msg(\"cannot receive messages\") continue } // add number of messages received from the queue wg.Add(len(resp.Messages)) // send received messages to sqs, so they can be processed for _, message := range resp.Messages { msgCh \u003c- message } // wait for workers in the pool to be finished. wg.Wait() } } } // startPool starts 10 goroutines which listens to the msgCh which receives the // messages from the SQS. func startPool(ctx context.Context, msgCh chan types.Message, wg *sync.WaitGroup) { for i := 0; i \u003c 10; i++ { go func() { for { select { case \u003c-ctx.Done(): return case msg, channelClosed := \u003c-msgCh: // If the channel is closed if !channelClosed { return } // handle the message here, insert your logic. // release the waitgroup to inform that the message has been processed. wg.Done() } } }() } } ","date":"2023-07-07","objectID":"/golang-sqs-consumer-worker-pool/:2:1","tags":null,"title":"Golang Sqs Consumer Worker Pool","uri":"/golang-sqs-consumer-worker-pool/"},{"categories":null,"content":"Some Points Let’s say you are receiving 1 million daily throughput from the SQS. For the 10 messages you received if you create 5 goroutines in each time in the end you will create 500_000 goroutines. if you create 5 goroutines which listens to a channel and process those messages, then you will only create 5 goroutines. Thanks for reading. Any feedback is appreciated. ","date":"2023-07-07","objectID":"/golang-sqs-consumer-worker-pool/:3:0","tags":null,"title":"Golang Sqs Consumer Worker Pool","uri":"/golang-sqs-consumer-worker-pool/"},{"categories":null,"content":"Introduction Go is an excellent programming language for building HTTP servers, thanks to its net/http package in the standard library, which makes it easy to attach HTTP handlers to any Go program. The standard library also includes packages that facilitate testing HTTP servers, making it just as effortless to test them as it is to build them. Nowadays, test coverage is widely accepted as an essential and valuable part of software development. Developers invest time in testing their code to get quick feedback when making changes, and a good test suite becomes an invaluable component of the software project when combined with continuous integration and delivery methodologies. Given the importance of a good test suite, what approach should developers using Go take when testing their HTTP servers? This article provides everything you need to know to test your Go HTTP servers thoroughly. ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:0:0","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"Http Server For Conversion of Roman Numerals We will have a web server which gives the roman numeral of the given number. We will only have 1 endpoint. Show the roman numeral of the number GET /roman ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:1:0","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"Example Request and Response Request curl --location --request GET 'http://localhost:8080/roman?query=1' Response { \"output\": \"I\" } ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:1:1","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"Code and Explanation package main import ( \"encoding/json\" \"log\" \"net/http\" \"strconv\" ) var ( nums = []int{1, 4, 5, 9, 10, 40, 50, 90, 100, 400, 500, 900, 1000} symbols = []string{\"I\", \"IV\", \"V\", \"IX\", \"X\", \"XL\", \"L\", \"XC\", \"C\", \"CD\", \"D\", \"CM\", \"M\"} ) func convertIntegerToRoman(input int) string { var ( i = len(nums) - 1 result string ) for input \u003e 0 { division := input / nums[i] input = input % nums[i] for division \u003e 0 { result += symbols[i] division = division - 1 } i = i - 1 } return result } type romanHandler struct{} func (h romanHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") if r.Method != http.MethodGet { http.Error(w, \"unsupported method\", http.StatusMethodNotAllowed) return } input := r.URL.Query().Get(\"query\") inputInt, err := strconv.Atoi(input) if err != nil { http.Error(w, \"invalid input\", http.StatusBadRequest) return } output := convertIntegerToRoman(inputInt) response := map[string]interface{}{ \"output\": output, } if err := json.NewEncoder(w).Encode(\u0026response); err != nil { return } } func main() { mux := http.NewServeMux() mux.Handle(\"/roman\", romanHandler{}) log.Fatal(http.ListenAndServe(\":8080\", mux)) } Points The function convertIntegerToRoman takes an integer and return the roman numeral conversion of the number. Please have a look on Convert Number Into Roman Numeral We accept a single query parameter named query in the URL which should have the number which will be converted. The struct implements the http.Handler interface by implementing the method of ServeHTTP(ResponseWriter, *Request) ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:1:2","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"Testing Of the Server The whole purpose of this blog was to learn how to test http servers in Go, so let’s find out. As we mentioned in the beginning Go has all of the tools we need to both create net/http and test net/http/httptest. All of the tools are included in the net module. Let’s create a file named main_test.go which has all of the tests for the HTTP Server. ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:2:0","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"Tests package main import ( \"fmt\" \"net/http\" \"net/http/httptest\" \"strings\" \"testing\" ) func TestRomanHandler(t *testing.T) { tt := []struct { name string httpMethod string query string responseBody string statusCode int }{ { name: \"unsupported httpMethod\", httpMethod: http.MethodPost, query: \"1\", responseBody: \"unsupported httpMethod\", statusCode: http.StatusMethodNotAllowed, }, { name: \"invalid input\", httpMethod: http.MethodGet, query: \"asd\", responseBody: `invalid input`, statusCode: http.StatusBadRequest, }, { name: \"correct query param\", httpMethod: http.MethodGet, query: \"1\", responseBody: `{\"output\":\"I\"}`, statusCode: http.StatusOK, }, } for _, tc := range tt { t.Run(tc.name, func(t *testing.T) { path := fmt.Sprintf(\"/roman?query=%s\", tc.query) request := httptest.NewRequest(tc.httpMethod, path, nil) responseRecorder := httptest.NewRecorder() romanHandler{}.ServeHTTP(responseRecorder, request) if responseRecorder.Code != tc.statusCode { t.Errorf(\"Want status '%d', got '%d'\", tc.statusCode, responseRecorder.Code) } if strings.TrimSpace(responseRecorder.Body.String()) != tc.responseBody { t.Errorf(\"Want '%s', got '%s'\", tc.responseBody, responseRecorder.Body) } }) } } To test the handler, we use the common table-driven approach and provide three cases: the http method is not correct http method is correct, but the query param is invalid both http method and query param is valid. For each case, we run a subtest that creates a new request and a response recorder. We use the httptest.NewRequest function to create an http.Request struct, which represents an incoming request to the handler. This allows us to simulate a real request without relying on an actual HTTP server. However, this function only handles the request half of the testing. To handle the response half, we use httptest.ResponseRecorder, which records the mutations of the http.ResponseWriter and enables us to make assertions on it later in the test. By using this duo of httptest.ResponseRecorder and http.Request, we can successfully test any HTTP handler in Go. Running the test will produce the following output. === RUN TestRomanHandler === RUN TestRomanHandler/unsupported_method === RUN TestRomanHandler/invalid_input === RUN TestRomanHandler/correct_query_param --- PASS: TestRomanHandler (0.00s) --- PASS: TestRomanHandler/unsupported_method (0.00s) --- PASS: TestRomanHandler/invalid_input (0.00s) --- PASS: TestRomanHandler/correct_query_param (0.00s) PASS ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:2:1","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"REFERENCES net/http Testing HTTP Servers By Ieftimov Converting Decimal To Roman ","date":"2023-03-05","objectID":"/testing-http-servers-in-go/:2:2","tags":["go","http","testing"],"title":"Testing HTTP Servers in Go","uri":"/testing-http-servers-in-go/"},{"categories":null,"content":"I have never written my long term goals into something and I just want to try it. Let’s see if I will be able to achieve my goals. Here is the list Read More Books (at least 25) Do more exercise 10000 pushups start to run regularly Write more blog posts at least 1 post per month write non-techincal stuff also (maybe some reviews on books) Go outside more 😄 Start to learn investing Get a promotion hopefully These are the things that comes to my mind. If anything comes up, I will add it to the list. Wish me luck! ","date":"2022-12-31","objectID":"/my-goals-for-2023/:0:0","tags":["goals","life"],"title":"My Goals For 2023","uri":"/my-goals-for-2023/"},{"categories":null,"content":"MongoDB \u0026 Golang Query Examples - Cheat Sheet This cheat sheet should help you about the MongoDB queries with Golang. We will start with some basic examples to more complex queries with Go Programming Language. The examples are written with Go 1.19 and go.mongodb.org/mongo-driver/mongo. ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:0:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Table Of Contents Connecting to MongoDB Inserting A Document to MongoDB Writing Multiple Documents To MongoDB Finding Single Document From MongoDB Finding All Documents From MongoDB Updating Document(s) From MongoDB Deleting Document(s) From MongoDB ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:1:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"How to Connect to MongoDB with Golang Connecting to MongoDB is fairly simple, you just connect the uri generated by the MongoDB. Then we can use the client.Database() function to make sure that we are connecting to the correct database. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") // disconnect the mongo client when main is completed defer func() { if err = client.Disconnect(ctx); err != nil { panic(err) } }() } To really make sure that we are connected to the correct database, we can use the Ping method. ctx, cancel = context.WithTimeout(context.Background(), 2*time.Second) defer cancel() err = client.Ping(ctx, readpref.Primary()) ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:2:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Inserting A Document to MongoDB with Golang To insert a document to MongoDB, we can use the bson.D provided by the MongoDB. But to make the operations more simple and more realistic to real world applications, we will use structs with bson tags. The model we are using is type Car struct { Id primitive.ObjectID `bson:\"_id\"` Brand string `bson:\"brand\"` Model string `bson:\"model\"` Year int `bson:\"year\"` } Then we can simply use the InsertOne() method to insert a document to MongoDB. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson/primitive\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) type Car struct { Id primitive.ObjectID `bson:\"_id\"` CreatedAt time.Time `bson:\"createdAt\"` Brand string `bson:\"brand\"` Model string `bson:\"model\"` Year int `bson:\"year\"` } func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") exampleData := Car{ Id: primitive.NewObjectID(), CreatedAt: time.Now().UTC(), Brand: \"Mercedes\", Model: \"G-360\", Year: 2002, } res, err := db.Collection(\"cars\").InsertOne(context.Background(), exampleData) if err != nil { log.Fatal(err) } // inserted id is ObjectID(\"639b62ae2518fbd9315e405d\") log.Printf(\"inserted id is %v\", res.InsertedID) } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:3:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Writing Multiple Documents To MongoDB with Golang We can use the InsertMany() method of the Collection object. However, the InsertMany() requires an []interface{} to work on. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson/primitive\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) type Car struct { Id primitive.ObjectID `bson:\"_id\"` CreatedAt time.Time `bson:\"createdAt\"` Brand string `bson:\"brand\"` Model string `bson:\"model\"` Year int `bson:\"year\"` } func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") var data []interface{} data = append(data, Car{ Id: primitive.NewObjectID(), CreatedAt: time.Now().UTC(), Brand: \"Toyota\", Model: \"Corolla\", Year: 2008, }) data = append(data, Car{ Id: primitive.NewObjectID(), CreatedAt: time.Now().UTC(), Brand: \"Ford\", Model: \"Focus\", Year: 2021, }) res, err := db.Collection(\"cars\").InsertMany(context.Background(), data) if err != nil { log.Fatal(err) } // 2 documents inserted log.Printf(\"%v documents inserted\", len(res.InsertedIDs)) } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:4:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Finding Single Document From MongoDB with Golang To find a single document with a condition, we can use the FindOne() method of *Collection object. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson/primitive\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) type Car struct { Id primitive.ObjectID `bson:\"_id\"` CreatedAt time.Time `bson:\"createdAt\"` Brand string `bson:\"brand\"` Model string `bson:\"model\"` Year int `bson:\"year\"` } func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") condition := bson.M{} cur, err := db.Collection(\"cars\").FindOne(context.Background(), condition) if err != nil { log.Fatal(err) } var data []Car if err := cur.All(context.Background(), \u0026data); err != nil { log.Fatal(err) } // now we can use the data array, which contains all of the documents for _, car := range data { log.Printf(\"the brand is %v\\n\", car.Brand) } } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:5:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Fetch the Lastly Created Document We can also pass mongo.Options to the Find() operation. Let’s say we want to fetch the lastly inserted document. we need to sort by the createdAt field it should be descending, that’s why we made the sort value as -1. var opts = options.FindOne().SetSort(bson.M{ \"createdAt\": -1, }) res := db.Collection(\"cars\").FindOne(context.Background(), bson.M{}, opts) if res.Err() != nil { log.Fatal(err) } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:5:1","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Finding All Documents From MongoDB with Golang To find the all documents in a collection, we can use the Find() method of *Collection object. In the below example, we did not specify any condition, which means that return all of the documents in the database. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson/primitive\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) type Car struct { Id primitive.ObjectID `bson:\"_id\"` CreatedAt time.Time `bson:\"createdAt\"` Brand string `bson:\"brand\"` Model string `bson:\"model\"` Year int `bson:\"year\"` } func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") condition := bson.M{} cur, err := db.Collection(\"cars\").Find(context.Background(), condition) if err != nil { log.Fatal(err) } var data []Car if err := cur.All(context.Background(), \u0026data); err != nil { log.Fatal(err) } // now we can use the data array, which contains all of the documents for _, car := range data { log.Printf(\"the brand is %v\\n\", car.Brand) } } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:6:0","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Finding Many Documents With Condition If we would like to return the cars where the brand is Toyota, then we can change the condition variable as condition := bson.M{ \"brand\": \"Toyota\" } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:6:1","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Use Projection in Find Operations If you want to use projection in Find() operation, we can use the mongo.Options for that. Let’s say we would like to return 2 fields return the brand of the car. return a boolean field to check if the car is new if the production year of the car is 2022, it is new else, it is old. SetProjection() sets the value for the projection field. var opts = options.Find().SetProjection( bson.M{ \"brand\": 1, \"isNew\": bson.M{ \"$cond\": bson.M{ \"if\": bson.M{\"$gte\": bson.A{\"$year\", 2022}}, \"then\": true, \"else\": false}, }, }) cur, err := db.Collection(\"cars\").Find(context.Background(), bson.M{}, opts) More will come, so please stay tuned! ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:6:2","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Update Single Document in MongoDB With Golang To update a single document, we should use the FindOneAndUpdate() or UpdateOne() operations. For this blog, we will use the FindOneAndUpdate() operation. package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") filter := bson.M{ \"brand\": \"Toyota\", \"model\": \"Corolla\", } update := bson.M{ \"year\": 2022, } res := db.Collection(\"cars\").FindOneAndUpdate(context.Background(), filter, update) if res.Err() != nil { log.Fatal(err) } // operation successful } How to return the updated document in MongoDB? We can use mongo.Options package to do that. We should set the return document option to after. opts := options.FindOneAndUpdate().SetReturnDocument(options.After) res := db.Collection(\"cars\").FindOneAndUpdate(context.Background(), filter, update, opts) // we can use the updated car document var updatedData Car if err := res.Decode(\u0026updatedData); err != nil { log.Fatal(err) } ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:6:3","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Delete Document(s) from MongoDB with Golang To delete a document we can use DeleteOne() method of the *Collection object. To delete many documents, we can use the DeleteMany() method of the *Collection package main import ( \"context\" \"log\" \"time\" \"go.mongodb.org/mongo-driver/bson\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\"mongodb://localhost:27017\")) if err != nil { log.Fatal(err) } db := client.Database(\"testdb\") filter := bson.M{ \"brand\": \"Toyota\", \"model\": \"Corolla\", } // for single document res, err := db.Collection(\"cars\").DeleteMany(context.Background(), filter) if err != nil { log.Fatal(err) } // 1 document is deleted. log.Printf(\"%v document is deleted\", res.DeletedCount) } More will come, so please stay tuned! ","date":"2022-12-15","objectID":"/golang-mongodb-query-examples/:6:4","tags":["golang","mongodb"],"title":"Golang \u0026 MongoDB Query Cheat Sheet","uri":"/golang-mongodb-query-examples/"},{"categories":null,"content":"Özet Bu yazıda basit bir kod parçasındaki bütün hataları bulup refactor edeceğim. Bunu yaparken de Go dilindeki temel unsurları açıklayarak yapacağım. Bu yazı Concurrency Made Easy videosundan ağır şekilde esinlenmiştir. Go dilinde concurreny baya öne çıkan bir unsur ancak doğru kullanmayı bilmek daha da önemli. Kendim de bu konuda mükemmel sayılmam ancak hala öğreniyorum. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:0:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Elimizdeki Fonksiyon Elimizdeki fonksiyon sadece bir parametre websites alıyor. Bu websiteler üzerinde gezinirken handle diye error döndüren bir fonksiyon alıyor ve handle fonksiyonu herhangi bir error döndürdüğü anda ise bu erroru döndürmek istiyor. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { wg.Done() \u003c-semaphores }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:1:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Sorunlar ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:0","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Semaphore ve WaitGroup Kısmı func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { wg.Done() \u003c-semaphores }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } Bu kısımlar kodumuzda bir panic oluşturmuyor, ancak aşağıdaki 2 durumdan birisi oluşuyor. \u003c-semaphores işlemi close(semaphores) işleminden önce oluşabilir ve bu durumda zaten kanaldan bir değer okur. close(semaphores) işlemi daha önce gerçekleşir ve \u003c-semaphores ise zero value alır. Önce wg.Done() operasyonu wg.Wait() fonksiyonun bitmesine ve close(semaphores) satırının çalışmasına yol açabilir. Her iki durumda da bir sıkıntı yok ancak bu kod fonksiyonun takibini daha zor yapıyor. Bunu go dilindeki şu tavsiyeyle çözebiliriz. Release locks and semaphores in the reverse order you acquired them. Anlamı ise locklar ve semaphoreları onları aldığınız sıranın tersinde bırakın. Bu durumda kodumuz şu hale geliyor ve daha basit bir duruma dönüşüyor. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } Şimdi ise sadece tek bir durum gerçekleşebilir o da \u003c-semaphores işlemi channel kapanmadan okuma işlemlerini yapabilir çünkü wg.Wait() işlemi ancak ve ancak bütün semaphores kanalından okuma işlemleri gerçekleştikten sonra gerçekleşebilir. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:1","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Semaphoreların Kullanımı Semaphoreların kullanıldığı kısıma biraz daha yakından bakalım. for _, website := range websites { semaphores \u003c- struct{}{} // semaphore acquire et go func() { defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } semaphores channelı 5 uzunluklu bir channel olduğundan dolayı 5 goroutine çalıştıktan sonra 6. taska geldiğinde fonksiyon 2. satırda duracak ve bu handle(website) fonksiyonu bitene kadar durmayacak. Halbuki şöyle bir durum daha mantıklı olabilir. Aynı anda 5 kez handle(website) fonksiyonu çalışsın, bir diğer deyimle goroutineler yaratılsın ve hazırda beklesin. Bunun için şu motto ile hareket edebiliriz. Acquire semaphores when you’re ready to use them. Anlamı ise semaphoreları ne zaman kullanmaya hazırsan o durumda acquire et. for _, website := range websites { go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } Bu değişiklikten sonra artık bütün goroutineler yaratılır ve aynı anda ancak 5 tanesi sadece handle(website) fonksiyonunu çalıştırabilir. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:2","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"For Loop For-range loop da yeni bir değişken website yaratıyoruz. Bir goroutine bu değişkeni updatelerken diğer goroutineler bu değişken üzerinden işlem yapıyor. Bundan dolayı burada bir data race var. Onun yerine 2 şekilde halledebiliriz. Functiona parametre olarak verme for _, website := range websites { go func(website string) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }(website) } Yeni Değişken Olarak Tanımlama for _, website := range websites { website := website go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() } Bundan ayrı olarak da genelde goroutineleri ayrı fonksiyonlara almak önerilir. Bu kod parçasını go func() { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } }() şu şekilde refactor edebiliriz. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { go worker(website, semaphores, \u0026wg, errChan) } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } func worker(website string, sem chan struct{}, wg *sync.WaitGroup, errChan chan err) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { errChan \u003c- err } } ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:3","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Error Channele Yazma Bütün bu işlemleri yaptık ancak hala kodumuzda bir sorun var. Herhangi bir goroutine errChan \u003c- err işlemini yaptığında diğer bütün error goroutineler bu kanala yazarken sonsuza kadar bekleyecekler ve bu da deadlock yaratacak. Bekleme sebebi errChan kanalının 1 uzunlukta bir channel olmasından dolayıdır. Bir goroutine başlatmadan önce ne zaman ve nasıl duracağını bilmek gerekir. Bunun yerine select ve case kullanarak sorunu halletmiş oluruz. func handleWebsites(websites []string) error { errChan := make(chan error, 1) semaphores := make(chan struct{}, 5) // aynı anda 5 iş çalıştır var wg sync.WaitGroup wg.Add(len(websites)) for _, website := range websites { go worker(website, semaphores, \u0026wg, errChan) } wg.Wait() close(semaphores) close(errChan) return \u003c-errChan } func worker(website string, sem chan struct{}, wg *sync.WaitGroup, errChan chan err) { semaphores \u003c- struct{}{} // semaphore acquire et defer func() { \u003c-semaphores wg.Done() }() if err := handle(website); err != nil { select { case errChan \u003c- err: default: } } } Bu durumda eğer herhangi bir goroutine errChane yazabilirse yazacak ve yazamazsa default case çalışacak. Hiçbir goroutine bloklanmayacak. Select Case ile blocking çağrıları non-blocking olarak değiştirebiliriz. ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:4","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"REFERENCES Concurrency Made Easy From Dave Chevey ","date":"2022-09-14","objectID":"/go-dilinde-concurrency-calisma-ornegi/:2:5","tags":["golang","concurrency","refactor"],"title":"Go Dilinde Concurrency Üzerinde Çalışma","uri":"/go-dilinde-concurrency-calisma-ornegi/"},{"categories":null,"content":"Bu yazıdaki bütün kodlar Bu repodan bulunmaktadır. Eğer demo versiyonunu görmek isterseniz http://banafilmoner.herokuapp.com/ sitesinden görebilirsiniz. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:0:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Gereksinimler Bu yazımızda yapacağımız siteyi eğer kendiniz de yapmak istiyorsanız Flask ve Scikit-learn kütüphanelerini yüklemeniz gerekmektedir. Bunları yüklemek için terminalden şu komutları yazabilirsiniz ya da her bir paketin dökümentasyonundan bakabilirsiniz. pip install Flask pip install scikit-learn ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:1:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Sitenin Yapısı Yapacağımız sitede film önerileri metin benzerliği ile olacak. Bu filmlerin açıklama metinlerini ise bir veri kümesinden alacağız. Bu veri kümesine TMDB 5000 Movies sayfasından ulaşabilirsiniz. Bundan dolayı önerebileceğimiz metinler sadece bu veri kümesindekiler olacaktır. Metin benzerliğini ise kosinüs benzerliği ile yapacağız. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:2:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Veri Seti ve Metin Benzerliği Veri setindeki title sütunu filmin başlığını ve overview sütunu ise filmi basitçe açıklar.Bu yazıda overview sütununu kullanarak metin benzerliğini kuracağız. Bunun için önce utils.py diye bir dosya oluşturalım ve indirdiğimiz veri setini de projedeki dosyaya koyalım. Öncelikle filmlerin açıklamalarını kullanarak kosinüs benzerliğini verecek olan bir fonksiyon yazalım. from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import linear_kernel def get_cosine_similarities(df): vectorizer = TfidfVectorizer(stop_words=\"english\") tf_idf_mat = vectorizer.fit_transform(df['overview']) cosine_sim = linear_kernel(tf_idf_mat, tf_idf_mat) return cosine_sim get_cosine_similarities(df) fonksiyonu parametere olarak DataFrame alır, DataFramei ise veri setini okuduktan sonra bu fonksiyona parametre olarak vereceğiz. Fonksiyonda kullanılan TfidfVectorizer metinlerden bilgi çıkarmamıza yarayan bir algoritmadır. Açılımı Term frequency (tf) -\u003e (terim sıklığı) ve inverse document frequency (ters döküman sıklığı)dır. Yani terimlerin her bir metinde ne kadar sıklıkla geçtiğine ve bu terimlerin bütün dökümanda ne kadar sıklıkla geçtiğine bakıp, hangi terimlerin cümleleri ayırmada önemli olduğuna karar verir. Bu bize (4803, n) boyutunda bir matrix dönderecektir. n ise bu algoritmanın bulduğu belirleyici kelimelerin sayısıdır. Yazdığımız fonksiyonla beraber, her bir cümle için her bir kelimenin ne kadar önemi olduğunu gösteren bir matrix elde edilecek. Daha sonra bu matrixi kullanarak her bir metin arasındaki benzerliği bulmak için linear_kernel kullanıyoruz. Bu algoritma ise bize (4803, 4803) boyutunda her bir metnin diğer 4038 filmin metni ile benzerliğini gösteren bir matrix döndürecek. Bu fonksiyondan çıkan sonuç ise şu şekildedir [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0.02160533 0. 0. ] [0. 0. 1. ... 0.01488159 0. 0. ] ... [0. 0.02160533 0.01488159 ... 1. 0.01609091 0.00701914] [0. 0. 0. ... 0.01609091 1. 0.01171696] [0. 0. 0. ... 0.00701914 0.01171696 1. ]] Görüldüğü gibi bazı değerler 0 bazıları 1 (köşegendekiler), bazıları da 0 ile 1 arasında. Kısaca Sonucu 0 olanlar arasında hiçbir benzerlik yok, 1 olanlar zaten kendileri ile ölçüldüğü için aynı olarak çıkıyor, örnek olarak 1.film ile 1.film arasındaki benzerlik 1 olacak doğal olarak 0-1 arasındakiler ise iki film arasındaki benzerliği gösteriyor. Ne yaptığımızı kısaca yazalım. Veri setini okuduk Kosinüs benzerlik matriksini oluşturduk. Şimdi yapılması gerekenler ise bu matrixi kullanıp film önerileri alabilmek. Bunun için yapılması gerekenler Kosinüs matrixini kullanıp bize verilen film için önerileri döndüren bir fonksiyon yazmak Flask ile web arayüzü oluşturup, kullanıcın girdiği filme öneriler vermek Bu fonksiyonu flask ile kullanabilmek. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:0","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Film Önerme Fonksiyonu Bu fonksiyona geçmeden önce veriyi okuyalım, ve kosinüs matriximizi alalım. Şunu belirtmem gerekir ki, kullanıcının attığı her requestte veri setini baştan okuyup kosinüs matrixini okumak yük olur. Bundan dolayı, bunu bir kez yapmak adına, bu işlemleri if __name__ == \"__main__\" altında yapacağız. Öncelikle bir app.py adında bir dosya açalım. Bu dosyada Flask applikasyonumuzun kodları olacak. Diğer utils.py dan fonksiyonları çağıracağız. app.py dosyasına şu kodları girelim. from flask import Flask, render_template, request, redirect import pandas as pd import utils app = Flask(__name__) if __name__== \"__main__\": df = pd.read_csv(\"data.csv\") df['overview'] = df['overview'].fillna('') df['lower_name'] = df['title'].str.lower() titles = pd.Series(df.index, index=df['lower_name']).drop_duplicates() cosine_sim = utils.get_cosine_similarities(df) app.run() Şuan app.py dosyasında yapılan işlemler. Flask uygulaması oluşturuldu. Veri okundu. Kosinüs benzerlik matriksi oluşturuldu. Main kısmında titles diye bir değişken oluşturulma sebebi bu değişkenin filmleri önerecek olan fonksiyonda kullanacağımızdan dolayıdır. Titles değişkeni tip olarak Seriesdir. Konsola yazdırdığımız zaman şöyle bir sonuç çıkacaktır. lower_name avatar 0 pirates of the caribbean: at world's end 1 spectre 2 the dark knight rises 3 john carter 4 ... el mariachi 4798 newlyweds 4799 signed, sealed, delivered 4800 shanghai calling 4801 my date with drew 4802 Length: 4803, dtype: int64 Şimdi filmleri önerecek fonksiyonu yazmaya başlayabiliriz. Bunu utils.py dosyasında yazalım. \"\"\" movie_title = istenilen filmin ismi cosine_similarity = kosinüs benzerlik matriksi titles= az önce oluşturduğumuz filmin isimlerine sahip olan `Series` df = bütün filmleri barındıran dataframe \"\"\" def get_recommendations(movie_title, cosine_similarity, titles, df): index_movie = titles[movie_title] #istenilen filmin indexini bul name_of_movie = df.iloc[index_movie]['title'] #daha sonra dataframeden filmin adını bul. #istenilen isim küçük harfli olabilir, biz #dataframde nasılsa onu almak için yapıyoruz. similarities = cosine_similarity[index_movie] #daha sonra girilen filmin kosinüs benzerlik #arrayini al, diğer filmlerle benzerlik arrayi similarity_scores = list(enumerate(similarities)) #işlem kolaylığı için her bir benzerliğin indexini #alabilmemiz lazım. yani (0, 0.2), (1, 0.4), (2. 0.7) ... gibi. similarity_scores = sorted(similarity_scores , key=lambda x: x[1], reverse = True) #bütün benzerlik skorlarını sırala similarity_scores = similarity_scores[1:11] #en benzer 10 filmi al similar_indexes = [x[0] for x in similarity_scores] #benzer filmlerin indexlerini al return df.iloc[similar_indexes], name_of_movie #benzer filmlerin bilgilerini almak için indexlerini kullan. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:1","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"HTML Arayüz Bu fonksiyon da yazıldığına göre şimdi Flask ile bağlayabiliriz. Ama öncelikle bir arayüzümüz olması gerekiyor. Bunun için aynı klasörde templates diye bir klasör oluşturun ve içine index.html adında bir dosya oluşturun. Bu dosya bizim kullanıcıdan arayüzü almamızı sağlayacak olan HTML kodunu içerecek. HTML kısmını anlatmayacağım. Basit şekilde Flask bildiğinizi varsayıyorum. index.html dosyasına buradaki arayüz kodunu yapıştırın. HTML kısmı şuan çok ilgi alanımız değil, eğer arayüz nasıl görünüyor diye merak ediyorsanız, buradan bakabilirsiniz. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:2","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"Flask Endpointleri halletme Bu kodda dikkatinizi çekmek istediğim bir nokta var. FORM bir ‘/’ yoluna POST request yapıyor. Flask uygulamasında ‘/’ adresine bir POST request yapılacak. Ayrıca websitesinin giriş sayfası da bu adrese GET request yapılarak alınacak. Şimdi app.py dosyasında bu koşulları sağlayan kodumuzu yazalım. from flask import Flask, render_template, request, redirect, flash, url_for import pandas as pd import utils app = Flask(__name__) @app.route('/', methods=['GET', 'POST']) def hello(): length = 0 movie_name = \"\" context = { #Bu dictionary önerilen filmlerin bilgilerini tutuyor. 'movies': [], #isimler 'urls': [], #filmlerin sayfaları 'release_dates': [], #filmlerin yayınlanma tarihleri 'runtimes': [], #filmlerin süreleri 'overviews': [] #filmleri anlatan metinler } if request.method == \"POST\": #Kullanıcı bir input girdiyse text = request.form['fname'].lower() print(\"request text\", text) try: recommended_df, movie_name = utils.get_recommendations( text, cosine_sim, titles, df) #girilen inputtan filmleri al context['movies'] = recommended_df.title.values context['urls'] = recommended_df.homepage.values context['release_dates'] = recommended_df.release_date.values context['runtimes'] = recommended_df.runtime.values context['overviews'] = recommended_df.overview.values length = len(context['movies']) except: return render_template('index.html', error=True) #filmi bulamadıysak error döndür. return render_template('index.html', length=length, context=context, movie_name=movie_name, error=False) if __name__ == '__main__': df = pd.read_csv(\"data.csv\") df['overview'] = df['overview'].fillna('') titles = pd.Series(df.index, index=df['lower_name']).drop_duplicates() cosine_sim = utils.get_cosine_similarities(df) app.run() Render templatede gönderilen context değişkeni HTML dosyasında parse ediliyor ve bilgiler güzel bir şekilde gösteriliyor. Dediğim gibi basit şekilde Flask bildiğiniz düşünüyorum. Beğendiyseniz paylaşırsanız çok sevinirim. İyi öğrenmeler. ","date":"2021-03-01","objectID":"/flask-ve-sklearn-ile-film-onerme-sitesi/:3:3","tags":["flask","makine ogrenmesi"],"title":"Flask ve Sklearn ile Film Önerme Sitesi Yapalım","uri":"/flask-ve-sklearn-ile-film-onerme-sitesi/"},{"categories":null,"content":"TANIM PyTorch da bulunan torch.autograd otomatik türev alma motoru şeklinde çalışır ve bu da nöral ağ eğitimini güçlendirir. Bu yazımızda belirli örnekler vererek konunun daha geniş şekilde anlaşılmasını sağlayacağız. Öncelikle çok kısa bir özetleyici metine bakalım. ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:1:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Arka Plan Nöral ağlar (neural networks) kendisine verilen veriyi belirli fonksiyonlarda işleyen bir bütündür. Bu fonksiyonların her biri bazı parametrelerden (ağırlıklar ve önyargı (weights and bias)) oluşur. Bu belirlenen parametrelere Pytorch da tensor adlı veri yapılarında tutulur. Bir Nöral ağın eğitilmesi iki kısımdan oluşur. Birinci kısımda sadece ileriye gidilir (forward propagation) ve ikinci kısımda geriye doğru gidilir (backward propagation). Peki bu ileri ve geri gitme işlemleri ne için yapılır onlara bakalım. İleriye Gitme (Yayılma) Bu kısımda nöral ağ kendisine verilen veriden en iyi tahminini yapmaya çalışır. Bu belirlenen veri, önceden bahsettiğimiz her bir fonksiyondan geçer ve en sonunda bir tahmin ortaya atılmış olur. Daha sonra belirlenen tahmin ve gerçek değer arasından bir kayıp (loss) değeri bulunur ve hatta bu değeri bulan fonksiyona da loss function denilir. Geriye Gitme (Yayılma) Bu kısımda ise nöral ağ, ilk bölümde hesaplanan kayıp veya hata değerini azaltmaya yönelik parametrelerinde iyileşmeye gider. Bunu yaparken de sonuçtan geriye dönük olarak her hata değerinin her bir parametreye bağlı olan türevini (derivative) hesaplar ve bu parametreleri, gradient descent kullanarak optimize eder. Ancak bu her bir fonksiyonun parametrelere göre türevini tek tek elimizle alamayız ve bize otomatik bir süreç lazım. İşte bu kısımda pytorch.autograd devreye giriyor ve bütün yükü alıyor. ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:1:1","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Autograd’da Türev Alma İşlemleri Şimdi autograd‘ın bütün bu değerleri nasıl kayıt ettiğine bakalım. Öncelikle iki tane a ve b tensor oluşturalım. Bu tensorları oluştururken parametre olan requires_grad parametresini True yapmamız gerekiyor aksi halde otomatik türev alma işlemleri gerçekleşemez çünkü bu parametre Tensorun grad adlı attributunda bu değerleri kayıt etmemize yardımcı oluyor. import torch x = torch.tensor([1., 2.], requires_grad=True) y = torch.tensor([2., 4.], requires_grad=True) Şimdi bu iki tensoru kullanarak yeni bir tensor z oluşturalım. Basit şekilde formül $$ z = 6x^2 - 2b^3 $$ z = 6*x**2 - 2*y**3 Şöyle varsayalım, x ve y bizim parametrelerimiz ve z bizim hata fonksiyonumuz olsun. Nöral ağ eğitiminde, hatanın bu parametreleri bağlı olan gradyantlarını (gradient) isteriz. PyTorch’da .backward() fonksiyonunu çağırdığımız zaman, auutograd her bir parametrenin (x, y) gradyantlarını bulur ve bunları her bir tensorun .grad attributunda kayıt eder. Öncelikle şuan x ve y nin grad değerlerine bakalım. print(\"X.grad = \", x.grad) print(\"Y.grad = \", y.grad) #Output X.grad = None Y.grad = None Ancak şimdi z tensorunda .backward() çağırdığımız zaman x ve y nin .grad attributularında z'nin kendilerine göre türevler yer alacak. Ancak z.backward() argümanını çağırabilmek için parametre olarak gradyant (gradient) vermemiz gerekiyor çünkü z bir vektör. Gradyant z ile aynı boyutlara sahip ve z’nin z ye göre türevini temsil eder. Şimdi z.backward() fonksiyonunu çağırabiliriz. gradyant_parametre = torch.tensor([1., 1.]) z.backward(gradient=gradyant_parametre) Şimdi x.grad ve y.grad değerleri oluşacak. Ancak bu değerleri görmeden önce kendimiz basit bir türev alalım. $$ \\frac{\\partial z}{\\partial x} = 12x $$ $$ \\frac{\\partial z}{\\partial y} = -6y^2 $$ Daha sonra bu kısmi türevlere x ve y tensorlarını koyduğumuz zaman ortaya çıkacak sonuçların şu şekilde olması lazım. print(\"x için = \", 12 * x) print(\"y için = \", -6 * y**2) x için = tensor([12., 24.], grad_fn=\u003cMulBackward0\u003e) y için = tensor([-24., -96.], grad_fn=\u003cMulBackward0\u003e)) Şimdi basit bir şekilde kontrol edelim. print(\"x.grad = \", x.grad) print(\"y.grad = \", y.grad) x.grad = tensor([12., 24.]) y.grad = tensor([-24., -96.]) Gördüğümüz üzere sonuçlar doğru çıkıyor. Üstte gözüken ggrad_fn=\u003cMulBackward0\u003e ise bu bu tensorun nasıl bir matematiksel operatör kullanarak oluşturulduğunu söylüyor. Eğer required_grad=False olsaydı bu değer None olurdu. Bütün yazdığımız operasyonlar için required_grad=True idi. Şimdi required_grad=False yapıp bir de öyle deneyelim. x = torch.tensor([1., 2.], requires_grad=False) y = torch.tensor([2., 4.], requires_grad=False) z = 6*x**2 - 2*y**3 gradyant_parametre = torch.tensor([1., 1.]) z.backward(gradient=gradyant_parametre) print(\"X.grad = \", x.grad) print(\"Y.grad = \", y.grad) Bu işlemden şöyle bir sonuç alacaksınız. RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn Bu da demek oluyor ki x ve y nin grad değerleri yok ve bundan dolayı grad_fn fonksiyonları da yok. x ve y nin grad değerleri olmadığı için z‘nin de grad değeri yok ve bu da hataya yol açıyor. Derin öğrenmede genellike önceden belirli datasetler ile eğitilmiş hazır modeller bulunmaktadır ve bunlara pretrained model denir. Bu modelleri kullanırken genellikle son katmana kadar olan bütün katmanların parametrelerini eğitmek istemeyiz çünkü bu işlem hem pahalı hem de çok da gerekli olmayan bir işlem. Bu parametreleri optimize etmeye çalışmadığımızdan dolayı bu parametrelerin required_grad değerleri False olacaktır. Örnek bir kod olarak da from torch import nn, optim model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False Burada Resnet18 modelinin parametrelini dondurma (freeze) işlemi yapılıyor ve böylece modeli kullanırken resnet18 modelini parametrelerinde herhangi bir optimize etme durumu söz konusu olmayacak. Ancak, sonradan eklenilen layerlarda optimize çal","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:2:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"REFERENCES Pytorch Tutorials ","date":"2021-02-20","objectID":"/pytorch-autograd-nedir-ve-nasil-calisir/:3:0","tags":["pytorch","matematik"],"title":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","uri":"/pytorch-autograd-nedir-ve-nasil-calisir/"},{"categories":null,"content":"Yazıya başlamadan önce belirmek isterim ki, bu tarz derin öğrenme terimlerinin İngilizce ile kullanılması taraftarıyım. Teknik terimlerin Türkçe karşılıkları genelde her zaman duymadığımız kelimeler oluyor ve internette Türkçe pek kaynak yok. Ondan dolayı ben bu terimlerin İngilizce öğrenilip, İngilizce kullanılması taraftarıyım. Herkes global olmaya çalışırken, bizim öyle davranmamamız için hiçbir sebep yok. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:0:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"TANIM Convolutional sinir ağları genel olarak sıradan sinir ağlarına çok benzerdir. Bu sinir ağları da öğrenebilir ağırlık (weight) ve önyargısı (bias) olan sinirlerden (neuron) oluşur. Her bir nöron bazı inputlar alır, dot product uygular ve bu işlemi lineer olmayan bir yolla devam ettirir. Bütün network hala tek bir ayırt edilebilir skoru açıklar. Network resim pixellerini alıp, sonda bir tahmin üretir. Networkun sonunda belirli bir kayıp fonksiyonu (loss function) bulunur. Peki bu convolutional sinir ağları normal sinir ağlarına bu kadar benziyorsa ne değişiyor? Bu sorunun cevabı ise şu şekildedir: Convolutional sinir ağları inputun resimlerden oluştuğunu varsayar, bu varsayım bize bazı özellikleri sisteme entegre etmemize yardımcı olur. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:1:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"YAPISAL GÖZLEM Normal Sinir Ağları: Normal sinir ağları tek bir input alır, onu bazı gizli katmanlardan (hidden layer) geçirir. Her bir hidden layer nöron kümelerinden oluşur, her bir nöron, bir önceki katmandaki bütün nöronlarla bağlantılıdır ve diğer nöronlardan bağımsız şekilde çalışır. Son katman ise sonuç katmanı (output layer) olarak adlandırılır ve bu katmanda her bir sınıfın olasılığı belli olur. Bu normal sinir ağları resimler kullanılınca pek iyi ölçeklenemiyor. Örnek olarak $(32, 32, 3)$ lük boyutlarda resimler kullanırsak, ilk katman $32 * 32 * 3 = 3072$ ağırlığa sahip olacaktır. Bu yük halledilebilir şekilde görülüyor ancak, bu fully-connected yapı büyük resimlere ölçeklenmiyor. Örnek olarak eğer biz boyutları $(200, 200, 3)$ olan resimler kullanırsak, bu sefer nilk nöronlar $200 * 200 * 3 = 120, 000$ ağırlığa sahip olacaklar. Ancak bu büyük numaralı ağırlıklar aşırı uyma (overfitting) denilen olaya sebep olacaktır. Convolutional sinir ağları ise inputun resimlerden oluşmasınından faydalanır ve buna göre yapıyı daha mantıklı şekilde kurar. Normal sinir ağlarının aksine, Convolutiona sinir ağlarının nöronları 3 boyuta ayarlanmış şekildedir. genişlik, yükseklik, derinlik. Örnek olarak $(32, 32, 3)$ boyutlu resimlerde Genişlik = 32 Yükseklik = 32 Derinlik = 3 olacaktır. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:2:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"PEKI BU CONVOLUTIONAL SINIR AĞLARI NASIL OLUŞTURULUYOR? Bu sinir ağları katman dizilerinden oluşur ve bu katmanlar ise şu şekildedir. Convolutional Katman Pooling Katmanı Fully-Connected Katmanı Bu 3 katmandan oluşan katmanları birleştirip bir sinir ağı oluşturacağız. CONVOLUTIONAL KATMAN Convolutional katman Convolutional sinir ağlarının büyük ağır işini yapan katmanlardır. Conv katmanlar parametreleri öğrenilebilir filtrelerden oluşur. Her bir filtre boyut olarak küçüktür, ancak input derinliği boyunca uzanırlar. Örnek olarak, tipik bir filtre $5 * 5 * 3$ boyutlarında olabilir. İlk 5 genişlik, ikinci 5 yükseklik ve üçüncü 3 ise resimin 3 derinlikli olmasından kaynaklanır. Doğrudan iletme kısmında, her bir filtreyi input resmi üzerinde kaydırıyoruz, bu kaydırma sırasında resimlerde pixeller ile filtredeki sayılar ile dot product alıyoruz. Filtreyi kaydırma işlemi sırasında 2 boyutlu bir aktivite haritası oluşturuyoruz. Bu harita ise bize her bir pozisyondaki cevabı veriyor. Sinir ağı, bu filtreler ne zaman belirli bir görsel özellik, örnek olarak kenar, gördüğü zaman öğrenecek. Her bir filtrenin oluşturduğu haritaları üst üste sıkıştırıp bunu bir sonraki katmana iletiyoruz. BOYUTSAL AYARLAMA Her bir nöronun nasıl bağlı olduğunu anlattık ancak output hacminde kaç tane nöron olduğundan bahsetmedik. Output hacmini belirleyen 3 ayrı parametre vardır. DERİNLİK: Bu parametre kaç tane nöron kullandığınıza işaret eder. Örnek olarak ilk convolutional katman input olarak resmi alırken, farklı nöronlar bu resimde farklı detayları fark edebilir. STRIDE (KAYDIRMA ADIMI): Bu parametre ise filtreyi kaç pixel kaydıracağımıza işaret eder. Eğer stride bir ise, filtreleri bir pixel kaydıracağımız anlamına gelir. ZERO-PADDING: (SIFIRLARLA DOLDURMA) Bazı durumlarda inputun etrafını sıfırlarla doldurmak uygun olmaktadır. Bu işlemin güzel bir tarafı ise, bize output boyutunu kontrol altında tutma olanağı vermesidir. Örnek olarak daha yüksek boyutlu outputlar istersek, inputu filtre boyutu kadar sıfırlarla doldurup, bir sonraki katmana aktarılacak outputun boyutunu, şuanki katmandaki input boyutuna eşit tutabiliriz. Output hacminin boyutunu şu şekilde hesaplayabiliriz. Input Boyutu = $W$ Convolutional katman nöronları filtre boyutu = $F$ Stride = $S$ Zero-Padding = $P$ Output hacmi boyutu formülü = $(W - F + 2P) / S + 1$. Örnek olarak eğer elimizde $10 * 10$ boyutlu bir resim varsa ve bizim filtre boyutumuz $3 * 3$, stride = $1$ ve padding = $0$ ise $$ Output Boyutu = (10 - 3 + 2*0) / 1 + 1 = 8 * 8 $$ Şimdi bu boyut tek bir nörondan çıkan sonuç. Eğer elimizde $n$ tane nöron varsa, bu katmandan çıkan sonucun boyutu $8 * 8 * n$ olacaktı. Yukarıdaki örnekten de görüleceği üzere filtre boyutumuz $3 3$, bundan dolayı resimde de (33) lük alanlar alıp, bu aldığımız alanla filtre arasında bir dot product işlemi uyguluyoruz. Peki resimdeki $31$ sayısına nasıl ulaştık onu inceleyelim. $$ (1 * 1) + (0 * 2) + (1 * 3) + (0 * 4) + (1 * 5) + (1 * 6) + (1 * 7) + (0 * 8) + (1 * 9) $$ $$ 1 + 3 + 5 + 6 + 7 + 9 = 31 $$ Özetlemek gerekirse Conv layer $W_1 * H_1 * D_1$ boyutlarında input alır 4 parametreye ihtiyaç duyar Filtre sayısı $K$ Filtrenin boyutları $F$ Stride $S$ Zero padding sayısı $P$ $W_2 * H_2 * D_2$ boyutlarında output üretir. $W_2 = (W_1 - F + 2P)/S + 1)$ $H_2 = (H_1 - F + 2P)/S + 1$ $D_2 = K$ ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:3:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"PYTHON İLE UFAK BİR GÖSTERİM Şimdi tensorflow ile basit bir gösterim yapıp bu boyutları daha iyi anlayalım. import tensorflow as tf # The inputs are 28x28 RGB images with `channels_last` and the batch # size is 4. input_shape = (4, 28, 28, 3) x = tf.random.normal(input_shape) y = tf.keras.layers.Conv2D( 2, 3, activation='relu', input_shape=input_shape[1:])(x) print(y.shape) (4, 26 , 26, 2) Burada olan işlemler şu şekildedir input_shape Conv layer’a verilecek olan inputun boyutlarıdır. (4, 28, 28, 3) şu anlama gelmektedir. Bizim elimizde 4 adet resim var, ve bu resimlerin boyutları (28, 28, 3)tür. Conv2D ’ e verilen parametreler ise şu şekildedir. İlk verilen parametre 2 kaç adet filtre kullanacağımızı gösterir. İkinci parametre 3 ise filtre boyutunu vermektedir. Yani filtre boyutumuz $(3, 3)$ olacaktır. Şimdi burada oluşan outputun nasıl oluştuğuna bakalım. Yukarıda özetlediğimiz gibi her şeyi tek tek yazalım Input boyutları $W_1 * H_1 * D_1$ şeklindeydi. Bundan dolayı $W_1 = 28$ $H_1 = 28$ $D_1 = 3$ Daha sonra filtre sayımız $K = 2$, filtre boyutumuz ise $F = 3$, stride ise default olarak $S = 1$dir. Padding ise default olarak $P = 0$dır. O zaman şimdi output boyutlarımızı $(W_2 * H_2 * D_2)$ hesaplayabiliriz. $W_2 = (28 - 3 + 2 * 0) / 1 + 1 = 25 + 1 = 26$ $H_2 = (28 - 3 + 2 * 0) / 1 + 1 = 25 + 1 = 26$ $D_2 = K = 2$ Her bir resim için oluşturulan output boyutları $(26, 26, 2)$. Elimizde 4 adet resim var ve bundan dolayı çıkan output boyutu $(4, 26, 26, 2)$ ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:3:1","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"POOLING LAYER Convolutional sinir ağlarında convolutional katmanlar arasına Pooling katmanları eklemek çok yaygındır. Pooling katmanının görevi verilen inputun boyutlarını kademeleri olarak azaltarak parametrelerin ve ağın işlem yükünün azaltımasını sağlamak. Bu şekilde aşırı uyma (overfitting) kontol altına alınmış olur. Pooling katmanı, bağımsız olarak çalışır ve her bir inputu Max operasyonu kullanarak boyutlarını azaltır. En yaygın Pooling katmanı, filtreleri $(2 * 2)$ boyutlarında olan ve inputu hem boydan ve hem enden ikiye bölenlerdir. Her bir Max operasyonu input olarak $(2 * 2)$ lik bir bölüm alacak ve bu 4 sayıdan en büyüğünü gönderecektir. Özetlemek gerekirse, Pooling katmanı input olarak $W_1 * H_1 * D_1$ boyutlarını kabul eder. İki parametreye ihtiyaç duyar Boyut $F$ Stride $S$ Boyutları $W_2 * H_2 * D_2$ olan output çıkarır. $W_2 = (W_1 - F)/S + 1$ $H_2 = (H_! - F)/S + 1$ $D_2 = D1$ Resimde de görüleceği üzere her bir $(2 * 2)$ lik bölümden en büyük sayılar alınıp yeni bir örnek elde ediliyor. Şimdi bunu Python ile kodlamaya çalışalım. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:4:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"Pooling Layer Python İle İmplementasyonu Bu layerı hem sıfırdan hem de kütüphane kullanarak kodlayabiliriz. Önce kütüphane kullanarak gösterelim. x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) x = tf.reshape(x, [1, 3, 3, 1]) max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid') max_pool_2d(x) Bu koddan çıkacak output ise \u003ctf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy= array([[[[5.], [6.]], [[8.], [9.]]]], dtype=float32)\u003e Çıkan sonucun nasıl çıktığını bence rahatlıkla yapabilirsiniz. Şimdi kendimiz sıfırdan bu layerı basit bir şekilde implement edelim. import numpy as np def pool2d(X, pool_size, mode='max'): p_h, p_w = pool_size #pool size ı al Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) #Outputu oluştur for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = X[i: i + p_h, j: j + p_w].max() #Her bir pool size kadar pixelin max'ını al return Y Şimdi kodumuzu yukarıda yazdığımız x arrayi ile test edersek, yine aynı sonucun çıkacağını göreceğiz. Bu yazımızda konuşulacaklar bu kadar. Beğendiyseniz paylaşmayı unutmayın. ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:4:1","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"REFERENCES https://anhreynolds.com/blogs/cnn.html https://cs231n.github.io/convolutional-networks/ https://cezannec.github.io/Convolutional_Neural_Networks/ https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D https://medium.com/ai-in-plain-english/pooling-layer-beginner-to-intermediate-fa0dbdce80eb ","date":"2020-12-17","objectID":"/evrisimsel-sinir-aglari-nedir/:5:0","tags":["numpy","pytorch","cnn"],"title":"Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir","uri":"/evrisimsel-sinir-aglari-nedir/"},{"categories":null,"content":"Makine öğrenmesinde modellerin veriyi görme şekli biz insanlardan farklıdır. Biz kolayca Kırmızı arabayı görüyorum. cümlesini anlayabilirken, model bu kelimeleri anlayacak vektörlere ihtiyaç duyar. Bu vektörlere word embeddings denir. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:0:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"WORD VECTORLERİ NASIL ÇALIŞIR - Tablodan Bak Her kelimemiz için belirli bir boyutta vektörümüz olacak ve bu vektörleri kelimeyi isteyerek alabiliriz. Buna key-value pair örneği verilebilir. key: kelime value: vektör Bundan dolayı herhangi bir kelimenin vektörüne bakmak için dictionaryden kelimeyi istediğimiz zaman vektöre ulaşmış olacağız. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:1:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Word2Vec: Tahmin Bazlı bir Metod Ana amacımız kelimelerden, kelime vektörleri oluşturmak. Word2Vec parametreli word vektörleri olan bir modeldir. Bu parametreler itaretive yöntemle, objective function(küçültmeye çalıştığımız fonksiyon) kullanarak optimize edilir. Peki bunu nasıl yapacağız. Unutmadan: amaç : her bir vektörü kelimenin içeriğini bilecek şekilde kodlamak nasıl yapılacak: vektörleri kelimelerden olası içerik tahmin edecek şekilde eğitmek. Word2Vec iterative bir metottur. Ana fikirleri kısaca şöyledir. büyük bir text corpusu alır texti, belirli bir sliding window(kayan pencere) kullanarak, her seferinde bir kelime ilerleyecek şekilde ilerlemek. Her bir adımda, bir tane central word (merkezi kelime) ve context words(içerik kelimeleri) -\u003e penceredeki diğer kelimeler. merkezi kelime için, içerik kelimelerinin olasılıklarını hesapla. vektörleri olasılıkları artıracak şekilde ayarla Resimde de görüleceği üzere her seferinde arkası mavi olan merkezi kelime ve diğerleri de içerik kelimeleri. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Objective Function (Amaç Fonksiyonu) Text corpusundaki her bir $ t = 1, … , T$ pozisyon için, Word2Vec merkezi kelimesi $w_{t}$ verilmiş m-boyutlu penceredeki içerik kelimelerini tahmin eder. $$ Likelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{t + j} \\mid w_t, \\theta) $$ Bu fonksiyonda $\\theta$ optimize edilecek bütün parametrelerdir. Amaç ve Kayıp Fonksiyonu $J(\\theta) ise ortalama negatif log olabilirlik fonksiyonudur. (Negative log-likelihood) $$ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0 } \\log P(w_{t + j} \\mid w_t, \\theta) $$ Bu formüldeki parçalara ayıralım. $\\sum_{t=1}^{T}$ Bu kısım bütün text üzerinde gezinir. $\\prod_{-m \\leq j \\leq m, y \\neq 0}$ bu ise kayma penceresini(sliding window) temsil eder. $\\log P(w_{t + j} \\mid w_t, \\theta)$ : bu ise merkezi kelimesi verilen içeriğin olasılığını hesaplar. Peki asıl sorulması gereken soru bu olasılıklar nasıl hesaplanacak? ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:1","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Olasılıkları Nasıl Hesaplayacağız? Hesaplamak istediğimiz olasılık $$ P(w_{t + j} \\mid w_t, \\theta) $$ Verilen her kelime $w$ için, iki adet vektörümüz var. $v_w$ -\u003e kelimenin merkezi kelime (central word) olduğu zaman $u_w$ -\u003e kelimenin içerik kelime (context word) olduğu zaman Vektörler train edildikten sonra, genel olarak içerik vektörlerini $u_w$ atar ve sadece merkezi kelime vektörlerini $v_w$ kullanılır. Bundan sonra verilen merkezi kelime $c$ ve içerik kelimesi $o$ kelimeleri için olasılık: $$ P(o \\mid c) = \\frac{exp(u_{o}^{T})}{\\sum_{v \\in V} exp(u_{w}^{T} v_c)} $$ NOT: Bu bir softmax fonksiyonudur. Softmax ile alakalı yazıma bu yazımdan ulaşabilirsiniz. Şimdi bu olasılıkları nasıl hesaplayacağımız gördüğümüze göre, vektörleri nasıl eğiteceğimizi görelim. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:2","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"NASIL EĞİTİLİR Kısaca bu sorunun cevabı Gradient Descent ile her seferinde bir kelime alarak gerçekleşir. Parametrelerimiz $\\theta$ bütün kelimelerin $v_w$ ve $u_w$ vektörleri olduğunu hatırlayalım. Bu vektörleri gradient descent kullanarak optimize edeceğiz. $$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta} J(\\theta) $$ Bu parametre güncellemerini her seferinde bir kelime kullanarak yapıyoruz. Her bir güncelleme bir merkez kelime ve içerik kelimesi ikilileriyle yapılır. Tekrardan kayıp fonksiyonuna bakalım. $$ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0 } \\log P(w_{t + j} \\mid w_t, \\theta) $$ Merkezi kelime $w_t$ için, kayıp fonksiyonu ayrı bir terimi her bir içerik kelimesi (w_{t + j}) (sliding window içerisindeki) (J_{t,j}(\\theta) = -\\log P(w_{t + j} \\mid w_t, \\theta\\ Bir örnek vererek bu durumu daha iyi anlayalım. Şu cümleyi ele alalım. Bugün bahçede bir top gördüm. Yeşil renkli bir kelimesi burada bizim merkezi kelimemizdir. Her seferinde bir kelimeye bakacağımız için, bir tane içerik kelimesi seçeceğiz. Örnek olarak top kelimesini ele alalım. Bundan sonra bu iki kelime için kayıp fonksiyonu Buradaki $V$ kümesi sliding windowu kapsayan kelimelerden oluşur. Loss (kayıp) fonksiyonumuzu aldığıma göre, şimdi vektörler üzerinde güncelleme yapalım. Burada hangi parameterlerin olduğuna göz atalım. merkezi kelime vektörlerinden sadece $v_{bir}$ içerik kelime vektörlerinden ise sliding window içerisindeki bütün kelimeler $u_w \\forall w \\in V$ Şuanki adımda sadece bu parametreler güncellenecek. $$ v_{bir} := v_{bir} - \\alpha \\frac{\\partial J_{t, j}(\\theta)}{\\partial v_{bir}} $$ $$ u_w = u_w - \\alpha \\frac{\\partial J_{t, j}(\\theta)}{\\partial u_{w}} \\forall w \\in V $$ Kayıp fonksiyonunu azaltacak şekilde yaptığımız her bir güncelleme, parametreler arasındaki benzerliği $v_{bir} \\hspace{1mm} ve \\hspace{1mm} u_{top}$ dot product’ını artırıyor ve aynı zamanda diğer her bir diğer $u_w$ ile $v_{bir}$ arasındaki benzerliği de azaltıyor. Bu biraz garip gelebilir ancak neden bir kelimesinin top kelimesinden hariç diğer kelimelerle benzerliğini azaltmaya çalışıyoruz. Diğerleri de mantıklı, içerik verecek kelimeler olabilir. Ancak bu bir sorun değil! Biz bu güncellemeyi her kelime için tek tek yaptığımızdan dolayı, yani her kelime bir kez merkezi kelime olacak, vektörler üzerindeki bütün güncellemelerin ortalaması metin içeriğininin dağılımını öğrenecektir. Bu yazıda partial derivative kısımlarına girilmemiştir. Ancak ben genel olarak Word2Vec modelinin nasıl çalıştığını anlatabildiğimi düşünüyorum. Eğer denemek isterseniz partial derivative kısımlarını kendiniz deneyebilirsiniz. Diğer yazılarda görüşmek üzere. Eğer yazıyı beğendiyseniz paylaşmayı unutmayın ki diğer insanlar da yararlansın. ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:2:3","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"REFERENCES https://lena-voita.github.io/nlp_course/word_embeddings.html ","date":"2020-12-14","objectID":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/:3:0","tags":["makine ogrenmesi","nlp"],"title":"Word2Vec Nedir ve Word2Vec Kelimelerden Nasıl Öğrenir","uri":"/word2vec-nedir-ve-word2veckelimelerden-nasil-ogrenir/"},{"categories":null,"content":"Problem Tanımı Knapsack problemi bilgisayar biliminde çok meşhur bir problemdir. Bu problemdeki amaç verilen ağırlık ve değerlerle en fazla değer toplayacak şekilde verilen ağırlık limitini aşmadan hangi itemlerin seçileceğidir. Knapscak problemi bir yüzyıldan fazla bir süredir, 1897 e kadar çalışmalar vardır. İsmini matematikçi Tobias Dantzig adlı matematikçinin eski çalışmalarından alır. Buradaki problemimiz için birden fazla yöntem vardır. Biz dinamik programlama ile bu problemin nasıl hallediliğine bakacağız. Şimdi problemdeki input ve istenilen outputa bakalım ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:0","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"INPUT Maksimum ağırlık limiti W ve elimizdeki paket sayısı n Ağırlıkların bulunduğu w[i] ve buna eş değer olan değer v[i] ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:1","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"OUTPUT Maksimum değer Hangi paketlerin alındığı ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:2","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"İMPLEMENTASYON Bu problemi analiz ederken algoritmanın hangi değerlere bağlı olacağını bulmaktır. Buradaki algoritmamız 2 ayrı değişkene dayanır. Bunların birincisi kaç tane paket taşıyacağımız ve elimizde kalan ağırlık limiti. Evet algoritmamızı iki değişkene bağlı şekilde yazacağız. Örnek olarak ilk 3 elemanı alarak, j maksimum limitli bir prpblemde optimum değer kaçtır. Buradaki ilk 3 eleman, hangi elemanları seçeceğimiz değişkenine örnektir. J limit ise ne kadar ağırlık limitimizin olduğudur. Bundan dolayı bir matrix oluşturup, her alt problemdeki optimum çözümü yazarsak bu şekilde istenilen sonuca ulaşabiliriz. Bundan dolayı [n+1][W+1] boyutlarında bir matrixte elimizdeki her alt alt problem için çözümleri saklayacağız. K[i][j] deki değer şu anlama geliyor: İlk i elemanı alarak j ağırlık limitli bir problemdeki optimum çözüm nedir. Peki bizim soruda ne isteniyordu? n elemanı kullanarak W limitli bir problemdeki çözüm nedir. Bundan dolayı bizim istediğimiz sonuç ise matrixin en son elemanı olan K[n][W] dir. Peki asıl soru olan her bu K[i][j] nasıl bulacağız. Öncelikle matriximizin ilk satırının hepsi 0 olacak. Bunun nedeni ise 0 item kullanırsak elde edebileceğimiz maksimum değer 0 dır. Diğer satırlarda ise durum farklıdır. Bundan dolayı her $1\\leq i \\leq n$ ve her $0 \\leq j \\leq W$ için bir durumu kontrol etmemiz gerekiyor. Kontrol etmemiz gereken değişken şuanki durumda yani item i ağırlığı şuanki j (ağırlık limiti) büyük mü. Çünkü eğer bizim şuanki itemimizin ağırlığı ağırlık limitimizden büyükse o itemi basitçe alamayız. Bu itemi alamadığımız için K[i][j] == K[i-1][j] olacak. Nedeni ise bu itemi seçmediğimiz için o durumdaki en optimum çözüm, o iteme kadar olanki en optimum çözüme eşittir. if w[i] \u003e j: K[i][j] = K[i-1][j] Eğer bu durum gerçekleşme ise elimizde iki seçenek var. Birinci seçenek şuanki itemi almamak. Bu durum yukarda bahsettiğimizin aynısı yani K[i][j] = K[i-1][j] İkinci seçenek ise bu itemi almak. Bu durumda elimizde olan optimum çözüm şu anlama geliyor. Şuanki itemin değeri v[i] Bu itemi aldığımız için geriye kalan j - w[i] ağırlık limitli ve ilk i-1 item arasındaki optimum çözüm, yani K[i-1][j-w[i]] Bu durumda ise optimum çözüm şu anlama geliyor. K[i][j] = v[i] + K[i-1][j - w[i]] Elimizde iki seçenek var. Peki hangisini seçeceğiz. Çok basit, en yüksek olan hangisi ise bunu seçeceğiz. Yani K[i][j] = max(K[i])[j], v[i] + K[i-1][j - w[i]]) Eğer bir basit bir kod yazmak istersek for j in range(W+1) K[0][j] = 0 //Yani İlk satırı 0 yap for i in range(1, n+1) for j in range(0, W + 1) if w[i] \u003e j // Eğer şuanki ağırlığımız şuanki limitten büyükse K[i][j] = 0 else K[i][j] = max(K[i-1][j], v[i] + K[i-1][j - w[i]]) Bizim çözümümüz ise K[n][W] deki değerdir. BackTracking kısmını sonra ekleyeceğim. ","date":"2020-11-18","objectID":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/:1:3","tags":["algoritmalar","python"],"title":"Dinamik Programlama ile Knapsack Problemi Nasıl Çözülür","uri":"/dinamik-programlama-ile-knapsack-problemi-nasil-cozulur/"},{"categories":null,"content":"NUMPY import numpy as np ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:0:0","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"NUMPY ARRAY Python’daki listelere çok benzerdir. Numoy arrayleri sadece aynı veri türüne sahip listeleri barındırabilir. Arrayler daha az hafızada yer kaplar. Array oluşturmak için yapmamız gereken a = np.array([1, 2, 3, 4]) type(a) numpy.ndarray Buna ek olarak da, arraylere tuple ekleyebiliriz. Örnek olarak a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(\"Birinci eleman {}\".format(a[0])) print(\"Birinci elemanın birinci elemanı {}\".format(a[0][0])) Birinci eleman [1 2 3] Birinci elemanın birinci elemanı 1 ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:0","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY ÖZELLİKLERİ Arrayin boyutlarını öğrenmek için yapmamız gereken **array.shape** yazmak olacaktır Arrayin rankini öğrenmek için yapmamız gereken **array.ndim** yazmak olacaktır Arrayin boyutunu öğrenmek için yapmamız gereken **array.size** yazmak olacaktır Arrayin barındırdığı veri tipini öğrenmek için yapmamız gereken **array.dtype** yazmak olacaktır print(\"Arrayin boyutları {}\".format(a.shape)) print(\"Arrayin ranki {}\".format(a.ndim)) print(\"Arraydeki eleman sayısı {}\".format(a.size)) print(\"Arrayin veri tipi {}\".format(a.dtype)) Arrayin boyutları (3, 3) Arrayin ranki 2 Arraydeki eleman sayısı 9 Arrayin veri tipi int64 ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:1","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY FONKSİYONLARI Arraylerden Sıfır Oluşturmak için yapmamız gereken shape yerine istediğimiz boyutları girmek. Numpy bizim için gerekli arrayi oluşturacaktır. shape = (2, 2) zeros = np.zeros(shape) zeros array([[0., 0.], [0., 0.]]) Arraylerden Bir Oluşturmak için yapmamız gereken shape yerine istediğimiz boyutları girmek. Numpy bizim için gerekli arrayi oluşturacaktır. shape = (2, 2) ones = np.ones(shape) ones array([[1., 1.], [1., 1.]]) İstediğimiz bir değerle istediğimiz boyutta bir array oluşturmak için ise np.full kullanıyoruz. a = np.full((6,5), 29) print(a) [[29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29] [29 29 29 29 29]] İdentity Matrix (Birim Matris) oluşturmak için ise np.eyeyazmak olacaktır. np.eye(3) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) Eğer belirli bir aralıkta belirli sayılarla artan bir array oluşturmak istiyorsak np.arange kullanmalıyız. rangearray = np.arange(10,100,10, dtype=float) rangearray array([10., 20., 30., 40., 50., 60., 70., 80., 90.]) Eğer yine belirli bir aralıkta değerler oluşturmak istiyorsak ve kaç tane oluşturacağımızı biliyorsak np.linspace kullanabiliriz. linarray = np.linspace(10, 100, 5) linarray array([ 10. , 32.5, 55. , 77.5, 100. ]) np.arange de 100 dahil değildi. Ancak np.linspace de dahil. Bunu da gözden kaçırmamak gerekir. Şimdi ise çok önemli bir fonksiyon olan np.reshape fonksiyonuna bakalım. Bu fonksiyon ile arraylerimizi istediğimiz formata çevirme şansımız var. array = np.arange(20) print(\"Önceki hali : \\n\", array) new_array = np.reshape(array, (4,5)) print(\"Sonraki hali : \\n\", new_array) Önceki hali : [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] Sonraki hali : [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]] ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:2","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"ARRAY INDEKSLEME (ARRAY INDEXING) Numpy arrayleri indekslemede çok kolaylık sağlıyor. Bir Boyutlu Array a1 = np.array([1, 3, 4, 5, 2, 10]) a1[0] 1 a1[4] 2 a1[-1] 10 a1[-3] 5 ÇOK BOYUTLU ARRAY a2 = np.array([[3, 4, 5, 6], [1, 3, 7, 2], [8, 4, 5, 10], [12, 124, 125, 126]]) a2[0] array([3, 4, 5, 6]) a2[0, 0] 3 a2[2, -1] # 2.indexin son elemanı 10 a2[2, 0] = 100 # 2.elemanın 0.elemanını 100 yap a2 array([[ 3, 4, 5, 6], [ 1, 3, 7, 2], [100, 4, 5, 10], [ 12, 124, 125, 126]]) a2[[0, 0, 2, 1]] # 0.index, 0.index, 2.index, 1.index array([[ 3, 4, 5, 6], [ 3, 4, 5, 6], [100, 4, 5, 10], [ 1, 3, 7, 2]]) a2[:2, ::2] # 2.satıra kadar 0 ile 2. indexler array([[3, 5], [1, 7]]) a2[::-1, ::-1] # Arrayi ters çevir array([[126, 125, 124, 12], [ 10, 5, 4, 100], [ 2, 7, 3, 1], [ 6, 5, 4, 3]]) a2[:, 0] # İlk sutün array([ 3, 1, 100, 12]) a2[0, :] # İlk satır array([3, 4, 5, 6]) I will ad other features to see how it is going ","date":"2020-11-09","objectID":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/:1:3","tags":["numpy"],"title":"Kapsamli Şekilde Python Numpy Öğrenelim","uri":"/python-numpy-nedir-ve-neden-numpy-kullanmaliyiz/"},{"categories":null,"content":"Merhaba bu yazımızda Makine Öğrenmesinde meşhur bir algoritma olan Knn algoritmasını sıfırdan yazacağız. Tabii ki hazır bir sürü kütüphane var ancak sıfırdan algoritmayı yazabilmek bize algoritmanın nasıl çalışacağını gösterecektir. Böylece Knn algoritması bir tahmin yaparken nasıl yapıyor olayın arkasında neler dönüyor bunları anlayabiliyor olacağız. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:0:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"K-Nearest Neighbour Nedir Öncelikle şunu bilmek gerekir ki K-Nearest-Neighbour adından da anlaşılacağı üzere en yakın k komşu noktalara bakıp en çok hangi label varsa o labelı tahmin(prediction) olarak verir. Peki bu yakınlık uzaklık ilişkisi nasıl kurulur önce ona bakalım. Uzaklığı ölçebilmek için belli başlı algoritmalar vardır. Bunlardan biri eucledian diğeri de manhattan uzaklığıdır. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Eucledian Uzaklığı Manhattan uzaklığında aslında iki nokta arasında uzaklığı alırken normal 2 boyutlu denklemde nasıl alıyorsak, bunun n boyutlu formüle döndürülmüş halidir. Örnek olarak $a = (x_1, y_1)$ ve $b = (x_2, y_2)$ olsun. Bu noktalar arasında uzaklığı bulurken yaptığımız işlem $$d(a, b) = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$ Peki eğer bizim verimiz n boyutlu olursa bu uzaklık nasıl ölçülecek?. Bu durumda ise uzaklık $$d(a,b)= \\sum_{i=1}^n (a_i - b_i)^2$$ Bu formulu ise Numpy ile şu şekilde yazabiliriz np.sqrt(np.sum(np.square(a - b), axis=1)) ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:1","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Manhattan Uzaklığı Manhattan uzaklığında iki nokta arasındaki uzaklık her bir alt noktanın farkının mutlak değerlerinin toplamı ile bulunur. Örnek olarak $a = (x_1, y_1)$ ve $b = (x_2, y_2)$ olsun. Bu noktalar arasında uzaklığı bulurken yaptığımız işlem $$d(a, b) = \\lvert x_1 - x_2\\rvert + \\lvert y_1 - y_2 \\rvert$$ Peki eğer bizim verimiz n boyutlu olursa bu uzaklık nasıl ölçülecek?. Bu durumda ise uzaklık $$d(a,b)= \\sum_{i=1}^n \\lvert a_i - b_i\\rvert$$ Bu formulu ise Numpy ile şu şekilde yazabiliriz np.sum(np.abs(a - b), axis=1) ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:1:2","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Algoritma Akışı KNN algoritmasında eğitme (training) işlemi aslında sadece verilen veriyi ezberlemekten ibarettir. Bundan dolayı eğitme kısmında bir şey yapmayacağız ancak tahmin etme (prediction) kısmında ise asıl üstteki formuülleri kullanıp işlem yapacağız. Bu algoritmayı Python ile Numpy Kullanarak implement edeceğiz. Önceklikle şu komut ile Numpy kütüphanesini import edelim. import numpy as np Şimdi Bir tane class tanımlayalım. Öncelikle kaç tane komşu kullanacağı modelin bir parametresi k olacak. Daha sonra hangi uzaklık formülünü kullanacağı da modelin bir parametresi olacak. Hadi başlayalım. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık Üstteki kod bloğunda yaptığımız işlem aslında modelin parametrelerini constructor fonskiyonunda tanımlamak oldu. Şimdi modelin eğitme fonksiyonunu yazalım. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık def fit(self, X, y): self.X = X self.y = y Yukarıda da bahsettiğimiz gibi Knn algoritması aslında sadece eğitme verisini ezberler. Bütün işlemler prediction kısmında yapılır. Bundan dolayı eğitme verisini modelin eğitim seti olarak değiştirebiliriz. Şimdi en önemli konu olan prediction kısmına gelelim. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:2:0","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"Tahmin Etme (Prediction) Kısmı Nasıl Olacak? Öncelikle uzaklıklar hesaplanacak Daha sonra en yakın k tane nokta alınacak Daha sonra bu en yakın k noktanın label sayılarını belirleyeceğiz. Yani hangi labeldan kaç tane olduğunu belirleyeceğiz. Daha sonra en çok sayısı olan label bizim tahminimiz olarak Diyelim ki bizim elimizde bir tane a verisi olsun ve eucledian uzaklık kullanıyor olalım. Bir nokta için nasıl bir işlem gerekiyor ona bakalım. #Uzaklık Hesaplama uzaklıklar = np.sqrt(np.sum(np.square(X - a), axis=1)) #En yakın k noktanın indexlerini bulalım en_yakın_k_index = np.argsort(uzaklıklar)[:k] #Şimdi bu en yakın k indexin hangi labellara ait olduğunu bulalım. en_yakın_labellar = y[en_yakın_k_index] #Daha sonra bu en yakın labellarda her labeldan kaç tane olduğunu bulalım labellar, adetler = np.unique(en_yakın_labellar, return_counts=True) #Daha sonra en çok hangi labelın sayısı var bunu bulalım max_label_index = np.argmax(adetler) #Daha sonra en çok sayısı olan label döndürelim return labellar[max_label_index] Şimdi burada bir sürü numpy fonksiyonu kullandık, bunlar kafa karıştırmış olabilir. Bundan dolayı bu fonksiyonlar ne işe yarıyor kısaca anlatayım. np.argsort(array): Bu fonksiyon parametre olarak aldığı arrayi sortlayacak indexleri verir. Aslında arrayi sortlamaz, ancak hangi indexler arrayi sortlar onu verir. np.unique(array, return_counts=True): Bu fonksiyon ise array içerisindeki unique elemanları dönderir. Eğer return_counts=True ise o zaman bu unique elemanlardan kaç tane var onu da gösterir. Örnek olarak array = [2, 3, 4, 3, 2, 10, 2] labellar, sayılar = np.unique(array, return_counts) print(labellar) #(2, 3, 4, 10) print(sayılar) #(3, 2, 1, 1) Labellar bizim arrayimizde hangi unique label var onları dönderir. Sayılar ise hangi labeldan kaç tane var onu dönderir. Örnek olarak 2 den 3 tane var. 10’dan 1 tane var. np.argmax(array): Bu fonksiyon array içerisindeki maximum elemanın indexini verir. Mesela yukarıdaki arrayde maximum eleman 10 ve 10’un indexi 5 dir. np.argmax() bu 5 indexini dönderir. Şimdi bu bir nokta içindi. Bunu test verimizde her bir nokta için yapalım. KNN Classının içerisinde implement edelim. class KNN: def __init__(self, k=2, uzaklık=\"eucledian\"): self.k = 2 self.uzaklık = uzaklık def fit(self, X, y): self.X = X self.y = y def predict(self, X_test): predictions = [] for point in X_test: if self.uzaklık == \"eucledian\": uzaklık = np.sqrt(np.sum(np.square(self.X - point), axis=1)) elif self.uzaklık == \"manhattan\": uzaklık = np.sum(np.abs(self.X - point), axis=1) indices = np.argsort(uzaklık)[:self.k] near_labels = self.y[indices] labels, values = np.unique(near_labels, return_counts=True) max_ind_label = np.argmax(values) prediction = labels[max_ind_label] predictions.append(prediction) return np.array(predictions) Yaptığımız işlemler her nokta için uzaklığı hesapladık en yakın k noktanın labellarını aldık bu labelların sayılarını öğrendik en çok label kimdeyse onu tahmin olarak öne sürdük Bu KNN modelini istediğiniz veride kullabilirsiniz. Daha kapsamlı koda bakmak isterseniz bu Github Repo’ya bakabilirsiniz. Bir sonraki yazıda görüşmek üzere. ","date":"2020-11-08","objectID":"/python-numpy-ile-sifirdan-knn-yazalim/:2:1","tags":["numpy","knn","makine ogrenmesi"],"title":"Python Numpy ile Sifirdan K Nearest Neighbours Algoritmasini Yazalim","uri":"/python-numpy-ile-sifirdan-knn-yazalim/"},{"categories":null,"content":"TANIM Softmax fonksiyonu modelden çıkan sonuçların olasılıksal şekilde ifade edilmesi için kullanılan bir fonksiyondur. Genellikle nöral ağlarda (neural network) ağın sonucunu sınıflara olasılık değerleri vermek için kullanılır. Softmax fonksiyonu input olarak $K$ boyutlu uzaydan vektör $z$ alır. Bu vektörü $K$ olasılık değerlerinden oluşan bir olasılık dağılımına çevirir. Bu olasılıkların her biri exponentialları ile doğru orantılıdır. Softmax fonksiyonu uygulamadan önce bu $z$ vektöründeki bazı değerler negatif de olabilir 0 da olabilir, pozitif de olabilir. Softmax fonksiyonunu uyguladıktan sonra ise bütün değerler $(0, 1)$ aralığında değer alır ve bütün değerlerin toplamı 1 olur. Standart softmax function tanımı şu şekildedir. $\\sigma : \\mathbb{R^{K}} \\rightarrow \\mathbb{R^K}$ Bir diğer deyişle bizim yaptığımız işlem her bir değerin exponential fonksiyonunu almak ve bunu toplama bölmek. Böylece normalize etmiş oluyoruz ve bütün değerleri topladığımız zaman sonuç 1 ediyor. Örnek olarak vektör $k = [1, 1, 1] \\in \\mathbb{R^3}$ olsun. O zaman, $$ \\sigma(k) = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}] $$ Peki bunu nasıl bulduk. Öncelikle toplamı hesaplayalım $$ \\sum_{j=1}^{K}e^{k_i} = e^1 + e^1 + e^1 = 3e $$ Toplam $3e$ çıktı. Şimdi her bir değeri exponential fonskiyona input olarak verirsek çıkacak sonuç $e^1 = e$ olur. Bizim softmax fonksiyonunda yaptığımız işlem ise bu değerleri alıp toplama bölmek. Yani, $$ \\frac{e}{3e} = \\frac{1}{3} $$ ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:1:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"NUMPY İLE NASIL İMPLEMENT EDİLİR Numpy fonksiyonunda arrayin direk exponential fonksiyonunu alabiliriz. Bunun için for loop açmamıza gerek yok import numpy as np arr = np.array([1, 3, 2]) exponential_arr = np.exp(arr) print(\"Array: {} \\nExponential Array: {} \\n\".format(arr, exponential_arr)) Array: [1 2 3] Exponential Array : [ 2.71828183 7.3890561 20.08553692] Arrayin direk üstel şekilde toplamını da alabiliriz. sum_of_exponentials = np.sum(exponential_arr) print(\"Exponential Array Toplamı: \", sum_of_exponentials) Exponential Array Toplamı: 30.19287485057736 Şimdi softmax implement etmek için her şeye sahibiz. Fonksiyon şeklinde implement edebiliriz. def softmax(arr): exp_array = np.exp(arr) exp_toplam = np.sum(exp_array) return exp_array/exp_toplam Şimdi fonksiyonumuzu test edelim arr = np.array([1, 1, 1]) softmax_array = softmax(arr) print(\"Array: {} \\nSoftmax Array: {}\".format(arr, softmax_array)) Array: [1 1 1] Softmax Array: [0.33333333 0.33333333 0.33333333] Gördüğümüz üzere softmax fonksiyondan çıkan arrayin toplamı 1 e eşit oluyor np.sum(softmax_array) #Sonuç 1 çıkıyor. Softmax fonksiyonu bu kadar. Bir sonraki yazıda görüşmek üzere. ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:2:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"REFERENCES wikipedia-softmax ","date":"2020-08-12","objectID":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/:3:0","tags":["numpy","derin ogrenme","matematik"],"title":"Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir","uri":"/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/"},{"categories":null,"content":"TANIM İstatistik, verilen bir örneklemden(sample) elde edilen herhangi bir değer demektir. İstatistiksel öğrenmede, verilen sampledan sağlanan bilgi ile karar verilir. İlk yaklaşımımız, sample’ın belirli bir dağılımdan (distribution) geldiğini farz ederek yapmak olacaktır. Bu dağılıma örnek olarak Gaussian dağılım verilebilir. Bu durumun avantajı ise, parametre sayısının azaltılması olacaktır. Tüm parametrelerimiz ortalama değer (mean) ve varyans (variance) olacaktır. Bu parametreleri sample tarafından elde ettikten sonra, bütün dağılımı biliyor olacağız. Bu parametreleri verilen sample üzerinden öğrenip, daha sonra bu bulduğumuz ortalama ve varyans değerlerini modele entegre ederek, tahmini bir dağılım elde edeceğiz. Daha sonra bu dağılımı da karar vermek için kullanacağız. Öncelikle olasılık kavramı diğer ismiyle density estimation (yoğunluk tahmini) anlamına gelen $p\\left(x\\right)$ kavramı ile başlıyoruz. Bu kavramı, Naive Bayesde de olduğu gibi tahmini olasılıkların $p(x \\mid C_{i})$, ve prior olasılık olan $P\\left(C_{i}\\right)$ olduğu ve bu olasılıkların daha sonra asıl amaç olan $P\\left(C_{i} \\mid x\\right)$‘i tahmin ederek sınıflandırma işlemi yapılması için kullanıyoruz. Peki bu parametreleri nasıl öğreneceğiz. Maksimum Likelihood Estimation kullanarak yapacağız. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:0","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"},{"categories":null,"content":"Maximum Likelihood Estimation (Maksimum Olasılık Tahmini) Elimizde birbirinden bağımsız ve aynı şekilde dağıtılmış olan bir sample var. Bu sample’ı $X = \\{ x^{t} \\}_{i=1}^{N}$ şeklinde gösterebiliriz. Bu sampledan çekilen her bir $x^{t}$ örneğin, bilinen bir olasılık dağılımına ait olduğunu varsayıyoruz. Bu olasılık dağılımını da $p\\left(x \\mid \\theta \\right)$ gösteriyoruz. $$ x^{t} \\sim p(x|\\theta) $$ Bizim buradaki amacımız bize en yüksek olasılığı $p\\left(x \\mid \\theta \\right)$ verecek olan $\\theta$ değerini bulmak. Bütün örnekler $x^{t}$ birbirinden bağımsız olduğundan parametre $\\theta$ nın olasılık fonksiyonu bütün verilen sampleların olasılıklarının çarpımına eşittir. $$ l(\\theta | X) = p(X|\\theta) = \\prod_{t=1}^{N}p(x^{i}|\\theta) $$ Maksimum olasılık tahmininde, bu değeri maksimum yapan $\\theta$ değerini bulmak istiyoruz. Bunu bulmak için önce logaritma alıp, daha sonra nerede maksimum yaptığına bakabiliriz. Logaritma alma sebebimiz ise logaritmanın çarpım sembolünü toplama çevirmesi ve başka kolaylıklar sağlaması dolayısıyladır. Log olasılık ise şöyle tanımlanır. $$ l(\\theta | X) \\equiv \\log l(\\theta | X) = \\sum_{t = 1}^{N}\\log p(x^{t}|\\theta) $$ Yazımızın başında bu her sample ın belirli bir dağılımdan geldiğini söylemiştik. Bunun için bir sürü seçenek olabilir. Bernouilli, Multinomial ve Gaussian(Normal) dağılımlar olabilir. Ancak biz burada sadece Gaussian(Normal) dağılım ile ilgilineceğiz. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:1","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"},{"categories":null,"content":"NORMAL DAĞILIMDA MAXİMUM LIKELIHOOD ESTIMATION X, ortalama yani $E[X] \\equiv \\mu$ ve varyans $Var(X) \\equiv \\sigma^{2}$ değerlerine sahip normal dağılımla dağıtılmış bir random variable olsun. O zaman density (yoğunluk) fonksiyonu şu şekilde $$ N(\\mu , \\sigma^{2}) = p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^{2}}} $$ O zaman verilen sampleın $X = \\{ x^t \\}_{t=1}^{N}$ log likelihood değeri de şu şekilde olur. $$ l(\\mu, \\sigma | X) = -\\frac{N}{2}\\log(2\\pi) - N \\log(\\sigma) - \\frac{\\sum_{t}(x^t - \\mu)^{2}}{2\\sigma^{2}} $$ Daha sonra sırayla bu fonksiyonun ortalama değer ve varyans için partial türevlerini alıp sıfıra eşitlediğimizde ortaya şöyle bir sonuç çıkıyor. $$ m = \\frac{\\sum_{t}x^t}{N} $$ $$ s^2 = \\frac{\\sum_{t}(x^t - m)^2}{N} $$ Burada $m$ ortalama değer için maximum likelihood estimate oluyor ve $s^2$ ise varyans için maximum likelihood estimate oluyor. Bu durumda istenilen parametreleri bulmuş olduk. Bundan sonraki yazıda ise bias(önyargı) ve Varyans(Variance) konuları işleyeceğiz. Sonraki yazılarda görüşmek üzere. ","date":"2020-01-12","objectID":"/makine-ogrenmesinde-parametrik-metodlar/:1:2","tags":["makine ogrenmesi","matematik"],"title":"Makine Ogrenmesinde Parametrik Metodlar","uri":"/makine-ogrenmesinde-parametrik-metodlar/"},{"categories":null,"content":"Hello there, Welcome to Hasan Ocak’s Blog. I recently graduated 🎓 from Sabanci University and I am currently working as Backend Developer 💻 in Turkey. I love reading and one day I decided to start a blog. I am not a consistent writer but I will try to write more. Until now I only written in Turkish but I will write in English too. Please stay tuned, Bye Here are my social links Github Linkedin Email ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]