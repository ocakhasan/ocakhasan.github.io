<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>derin ogrenme - Tag - Hasan Ocak Tech Blog</title><link>https://ocakhasan.github.io/tags/derin-ogrenme/</link><description>derin ogrenme - Tag - Hasan Ocak Tech Blog</description><generator>Hugo -- gohugo.io</generator><language>tr</language><managingEditor>hasanocak.tech@gmail.com (Hasan Ocak)</managingEditor><webMaster>hasanocak.tech@gmail.com (Hasan Ocak)</webMaster><lastBuildDate>Wed, 12 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://ocakhasan.github.io/tags/derin-ogrenme/" rel="self" type="application/rss+xml"/><item><title>Softmax Aktivasyon Fonksiyonu Nedir ve Numpy ile Nasıl Implement Edilir</title><link>https://ocakhasan.github.io/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/</link><pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate><author>Hasan Ocak</author><guid>https://ocakhasan.github.io/softmax-aktivasyon-fonksiyonu-nedir-numpy-implementasyonu/</guid><description>Derin Öğrenmede kullanılan softmax aktivasyon fonksiyonunu inceliyoruz.</description></item></channel></rss>