<!doctype html><html lang=tr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Pytorch AutoGrad Nedir ve Nasıl Çalışır - Hasan Ocak Tech Blog</title><meta name=Description content="Çeşitli konulardan bahseden bir tech blog"><meta property="og:title" content="Pytorch AutoGrad Nedir ve Nasıl Çalışır"><meta property="og:description" content="Basit şekilde Pytorch Autograd ile otomatik olarak nasıl türev işlemleri halledilir."><meta property="og:type" content="article"><meta property="og:url" content="https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/"><meta property="og:image" content="https://ocakhasan.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-20T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-07T21:09:02+03:00"><meta property="og:site_name" content="Hasan Ocak Tech Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ocakhasan.github.io/logo.png"><meta name=twitter:title content="Pytorch AutoGrad Nedir ve Nasıl Çalışır"><meta name=twitter:description content="Basit şekilde Pytorch Autograd ile otomatik olarak nasıl türev işlemleri halledilir."><meta name=application-name content="Hasan Ocak Tech Blog"><meta name=apple-mobile-web-app-title content="Hasan Ocak Tech Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/><link rel=prev href=https://ocakhasan.github.io/evrisimsel-sinir-aglari-nedir/><link rel=next href=https://ocakhasan.github.io/flask-ve-sklearn-ile-film-onerme-sitesi/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><meta name=google-site-verification content="6FsPs8qV_9zEnPMWf3bEQwRJFU2Q9imUdyZqX_99So4"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Pytorch AutoGrad Nedir ve Nasıl Çalışır","inLanguage":"tr","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/ocakhasan.github.io\/pytorch-autograd-nedir-ve-nasil-calisir\/"},"genre":"posts","keywords":"pytorch, matematik","wordcount":827,"url":"https:\/\/ocakhasan.github.io\/pytorch-autograd-nedir-ve-nasil-calisir\/","datePublished":"2021-02-20T00:00:00+00:00","dateModified":"2022-12-07T21:09:02+03:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Hasan Ocak"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Hasan Ocak Tech Blog">Hasan Ocak</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about>About </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Hasan Ocak Tech Blog">Hasan Ocak</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about title>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Pytorch AutoGrad Nedir ve Nasıl Çalışır</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Hasan Ocak</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2021-02-20>2021-02-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;827 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#tanim>TANIM</a><ul><li><a href=#arka-plan>Arka Plan</a><ul><li><a href=#ileriye-gitme-yayılma><strong>İleriye Gitme (Yayılma)</strong></a></li><li><a href=#geriye-gitme-yayılma><strong>Geriye Gitme (Yayılma)</strong></a></li></ul></li></ul></li><li><a href=#autogradda-türev-alma-işlemleri>Autograd&rsquo;da Türev Alma İşlemleri</a></li><li><a href=#references>REFERENCES</a></li></ul></nav></div></div><div class=content id=content><h2 id=tanim>TANIM</h2><p><a href=https://pytorch.org/ target=_blank rel="noopener noreffer">PyTorch</a> da bulunan <code>torch.autograd</code> otomatik türev alma motoru şeklinde çalışır ve bu da nöral ağ eğitimini güçlendirir. Bu yazımızda belirli örnekler vererek konunun daha geniş şekilde anlaşılmasını sağlayacağız. Öncelikle çok kısa bir özetleyici metine bakalım.</p><h3 id=arka-plan>Arka Plan</h3><p>Nöral ağlar (neural networks) kendisine verilen veriyi belirli fonksiyonlarda işleyen bir bütündür. Bu fonksiyonların her biri bazı <em>parametrelerden</em> (ağırlıklar ve önyargı (weights and bias)) oluşur. Bu belirlenen parametrelere Pytorch da <code>tensor</code> adlı veri yapılarında tutulur. Bir Nöral ağın eğitilmesi iki kısımdan oluşur. Birinci kısımda sadece ileriye gidilir (forward propagation) ve ikinci kısımda geriye doğru gidilir (backward propagation). Peki bu ileri ve geri gitme işlemleri ne için yapılır onlara bakalım.</p><h4 id=ileriye-gitme-yayılma><strong>İleriye Gitme (Yayılma)</strong></h4><p>Bu kısımda nöral ağ kendisine verilen veriden en iyi tahminini yapmaya çalışır. Bu belirlenen veri, önceden bahsettiğimiz her bir fonksiyondan geçer ve en sonunda bir tahmin ortaya atılmış olur. Daha sonra belirlenen tahmin ve gerçek değer arasından bir kayıp (loss) değeri bulunur ve hatta bu değeri bulan fonksiyona da <code>loss function</code> denilir.</p><h4 id=geriye-gitme-yayılma><strong>Geriye Gitme (Yayılma)</strong></h4><p>Bu kısımda ise nöral ağ, ilk bölümde hesaplanan kayıp veya hata değerini azaltmaya yönelik parametrelerinde iyileşmeye gider. Bunu yaparken de sonuçtan geriye dönük olarak her hata değerinin her bir parametreye bağlı olan türevini (derivative) hesaplar ve bu parametreleri, <code>gradient descent</code> kullanarak optimize eder. Ancak bu her bir fonksiyonun parametrelere göre türevini tek tek elimizle alamayız ve bize otomatik bir süreç lazım.</p><p>İşte bu kısımda <code>pytorch.autograd</code> devreye giriyor ve bütün yükü alıyor.</p><h2 id=autogradda-türev-alma-işlemleri>Autograd&rsquo;da Türev Alma İşlemleri</h2><p>Şimdi <code>autograd</code>&lsquo;ın bütün bu değerleri nasıl kayıt ettiğine bakalım.</p><p>Öncelikle iki tane <code>a</code> ve <code>b</code> <code>tensor</code> oluşturalım. Bu tensorları oluştururken parametre olan <code>requires_grad</code> parametresini <code>True</code> yapmamız gerekiyor aksi halde otomatik türev alma işlemleri gerçekleşemez çünkü bu parametre Tensorun <code>grad</code> adlı attributunda bu değerleri kayıt etmemize yardımcı oluyor.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>2.</span><span class=p>,</span> <span class=mf>4.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Şimdi bu iki tensoru kullanarak yeni bir tensor <code>z</code> oluşturalım.</p><p>Basit şekilde formül</p><p>$$
z = 6x^2 - 2b^3
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=mi>6</span><span class=o>*</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>**</span><span class=mi>3</span>
</span></span></code></pre></td></tr></table></div></div><p>Şöyle varsayalım, <code>x</code> ve <code>y</code> bizim parametrelerimiz ve <code>z</code> bizim <strong>hata fonksiyonumuz</strong> olsun. Nöral ağ eğitiminde, hatanın bu parametreleri bağlı olan gradyantlarını (gradient) isteriz.</p><p>PyTorch&rsquo;da <code>.backward()</code> fonksiyonunu çağırdığımız zaman, <code>auutograd</code> her bir parametrenin (x, y) gradyantlarını bulur ve bunları her bir tensorun <code>.grad</code> attributunda kayıt eder. Öncelikle şuan <code>x</code> ve <code>y</code> nin grad değerlerine bakalım.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;X.grad = &#34;</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Y.grad = &#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#Output</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span>  <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=n>Y</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span>  <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><p>Ancak şimdi <code>z</code> tensorunda <code>.backward()</code> çağırdığımız zaman <code>x</code> ve <code>y</code> nin <code>.grad</code> attributularında <code>z'nin</code> kendilerine göre türevler yer alacak. Ancak <code>z.backward()</code> argümanını çağırabilmek için parametre olarak <code>gradyant (gradient)</code> vermemiz gerekiyor çünkü z bir vektör. <code>Gradyant</code> <code>z</code> ile aynı boyutlara sahip ve z&rsquo;nin z ye göre türevini temsil eder. Şimdi <code>z.backward()</code> fonksiyonunu çağırabiliriz.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gradyant_parametre</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>z</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>gradient</span><span class=o>=</span><span class=n>gradyant_parametre</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Şimdi <code>x.grad</code> ve <code>y.grad</code> değerleri oluşacak. Ancak bu değerleri görmeden önce kendimiz basit bir türev alalım.</p><p>$$
\frac{\partial z}{\partial x} = 12x
$$</p><p>$$
\frac{\partial z}{\partial y} = -6y^2
$$</p><p>Daha sonra bu kısmi türevlere <code>x</code> ve <code>y</code> tensorlarını koyduğumuz zaman ortaya çıkacak sonuçların şu şekilde olması lazım.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;x için = &#34;</span><span class=p>,</span> <span class=mi>12</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y için = &#34;</span><span class=p>,</span> <span class=o>-</span><span class=mi>6</span> <span class=o>*</span> <span class=n>y</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x için =  tensor([12., 24.], grad_fn=&lt;MulBackward0&gt;)
</span></span><span class=line><span class=cl>y için =  tensor([-24., -96.], grad_fn=&lt;MulBackward0&gt;))
</span></span></code></pre></td></tr></table></div></div><p>Şimdi basit bir şekilde kontrol edelim.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;x.grad = &#34;</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y.grad = &#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span>  <span class=n>tensor</span><span class=p>([</span><span class=mf>12.</span><span class=p>,</span> <span class=mf>24.</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span>  <span class=n>tensor</span><span class=p>([</span><span class=o>-</span><span class=mf>24.</span><span class=p>,</span> <span class=o>-</span><span class=mf>96.</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Gördüğümüz üzere sonuçlar doğru çıkıyor. Üstte gözüken <code>ggrad_fn=&lt;MulBackward0></code> ise bu bu tensorun nasıl bir matematiksel operatör kullanarak oluşturulduğunu söylüyor. Eğer <code>required_grad=False</code> olsaydı bu değer <code>None</code> olurdu.</p><p>Bütün yazdığımız operasyonlar için <code>required_grad=True</code> idi. Şimdi <code>required_grad=False</code> yapıp bir de öyle deneyelim.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>2.</span><span class=p>,</span> <span class=mf>4.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=mi>6</span><span class=o>*</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>**</span><span class=mi>3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>gradyant_parametre</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>z</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>gradient</span><span class=o>=</span><span class=n>gradyant_parametre</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;X.grad = &#34;</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Y.grad = &#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Bu işlemden şöyle bir sonuç alacaksınız.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</span></span></code></pre></td></tr></table></div></div><p>Bu da demek oluyor ki <code>x</code> ve <code>y</code> nin grad değerleri yok ve bundan dolayı <code>grad_fn</code> fonksiyonları da yok. <code>x</code> ve <code>y</code> nin grad değerleri olmadığı için <code>z</code>&lsquo;nin de grad değeri yok ve bu da hataya yol açıyor.</p><p>Derin öğrenmede genellike önceden belirli datasetler ile eğitilmiş hazır modeller bulunmaktadır ve bunlara <em>pretrained model</em> denir. Bu modelleri kullanırken genellikle son katmana kadar olan bütün katmanların parametrelerini eğitmek istemeyiz çünkü bu işlem hem pahalı hem de çok da gerekli olmayan bir işlem. Bu parametreleri optimize etmeye çalışmadığımızdan dolayı bu parametrelerin <code>required_grad</code> değerleri <code>False</code> olacaktır. Örnek bir kod olarak da</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span><span class=p>,</span> <span class=n>optim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Freeze all the parameters in the network</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span></code></pre></td></tr></table></div></div><p>Burada <a href=https://pytorch.org/hub/pytorch_vision_resnet/ target=_blank rel="noopener noreffer">Resnet18</a> modelinin parametrelini dondurma (freeze) işlemi yapılıyor ve böylece modeli kullanırken <code>resnet18</code> modelini parametrelerinde herhangi bir optimize etme durumu söz konusu olmayacak. Ancak, sonradan eklenilen layerlarda optimize çalışması yapılabilir o kadar.</p><h2 id=references>REFERENCES</h2><p><a href=https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py target=_blank rel="noopener noreffer">Pytorch Tutorials</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-12-07</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/pytorch-autograd-nedir-ve-nasil-calisir/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/ data-title="Pytorch AutoGrad Nedir ve Nasıl Çalışır" data-hashtags=pytorch,matematik><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/ data-hashtag=pytorch><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/ data-title="Pytorch AutoGrad Nedir ve Nasıl Çalışır"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/ data-title="Pytorch AutoGrad Nedir ve Nasıl Çalışır"><i data-svg-src=/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://ocakhasan.github.io/pytorch-autograd-nedir-ve-nasil-calisir/ data-title="Pytorch AutoGrad Nedir ve Nasıl Çalışır"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/pytorch/>pytorch</a>,&nbsp;<a href=/tags/matematik/>matematik</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/evrisimsel-sinir-aglari-nedir/ class=prev rel=prev title="Evrişimsel Sinir Ağları  (Convolutional Neural Network) Nedir"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Evrişimsel Sinir Ağları (Convolutional Neural Network) Nedir</a>
<a href=/flask-ve-sklearn-ile-film-onerme-sitesi/ class=next rel=next title="Flask ve Sklearn ile Film Önerme Sitesi Yapalım">Flask ve Sklearn ile Film Önerme Sitesi Yapalım<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.111.2">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2019 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Hasan Ocak</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/katex/katex.min.css><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-165674239-4",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=UA-165674239-4" async></script></body></html>